{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../../datasets/objective_subjective/'\n",
    "subjective_file = 'quote.tok.gt9.5000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subjective: 5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'smart and alert , thirteen conversations about one thing is a small gem . \\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open(path + subjective_file, 'r', encoding='utf-8', errors='ignore')\n",
    "subjective_lines = []\n",
    "for line in f:\n",
    "    subjective_lines.append(line)\n",
    "f.close()\n",
    "\n",
    "print('Subjective:',len(subjective_lines))\n",
    "subjective_lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective: 5000\n"
     ]
    }
   ],
   "source": [
    "objective_file = 'plot.tok.gt9.5000'\n",
    "f = open(path + objective_file, 'r', encoding='utf-8', errors='ignore')\n",
    "objective_lines = []\n",
    "for line in f:\n",
    "    objective_lines.append(line.rstrip())\n",
    "f.close()\n",
    "\n",
    "print('Objective:',len(objective_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the movie begins in the past where a young boy named sam attempts to save celebi from a hunter'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objective_lines[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tf_idf = f(sentence,term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'the movie begins in the past where a young boy named sam attempts to save celebi from a hunter'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_lines = objective_lines + subjective_lines\n",
    "print(len(all_lines))\n",
    "all_lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.matrixlib.defmatrix.matrix'>\n",
      "20893\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(max_df=0.99) # at 0.99, same result as 1: \n",
    "sentence_tfidf = tfidf.fit_transform(all_lines)\n",
    "vocab = tfidf.vocabulary_\n",
    "if False:\n",
    "    tfidf_mat = sentence_tfidf\n",
    "else:\n",
    "    tfidf_mat = sentence_tfidf.todense()\n",
    "print(type(tfidf_mat))\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['93',\n",
       " '94',\n",
       " '95',\n",
       " '996',\n",
       " '_boogie',\n",
       " 'aaa',\n",
       " 'aaliyah',\n",
       " 'abandon',\n",
       " 'abandone',\n",
       " 'abandoned']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(vocab)[258:268]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_mat[:2,270:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train and CV sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.concatenate((np.zeros((5000,), dtype=int), np.ones((5000,), dtype=int)))\n",
    "print(labels.shape)\n",
    "labels[4990:5010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(\n",
    "    tfidf_mat, labels, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 20893) (2000, 20893) (8000,) (2000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_cv.shape, y_train.shape, y_cv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N_TREES = 300\n",
    "# LEARN_RATE = 0.2\n",
    "# MIN_IN_LEAF = 10\n",
    "# CV score: 0.8285\n",
    "\n",
    "# 107 trees\n",
    "# LEARN_RATE = 0.2\n",
    "# MAX_DEPTH = 8\n",
    "# MIN_IN_LEAF = 5 #7\n",
    "# MAX_FEATURES = 'sqrt'\n",
    "# CV score: 0.8415\n",
    "\n",
    "# Gradient Boosting Classifier parameters\n",
    "N_TREES = int(round(np.sqrt(X_train.shape[0]) * 1.2))\n",
    "# N_TREES = 500\n",
    "LEARN_RATE = 0.2\n",
    "MAX_DEPTH = 8\n",
    "MIN_IN_LEAF = 5 #7\n",
    "MAX_FEATURES = 'sqrt'\n",
    "N_TREES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[[0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[0])\n",
    "print(X_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.863625\n",
      "CV score: 0.8415\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# loss: deviance: logistic log likelihood\n",
    "gbc = GradientBoostingClassifier(learning_rate=LEARN_RATE, \n",
    "                                   n_estimators=N_TREES, \n",
    "                                   min_samples_leaf=MIN_IN_LEAF,\n",
    "                                   max_features=MAX_FEATURES)\n",
    "gbc.fit(X_train, y_train)\n",
    "print('Train score:', gbc.score(X_train, y_train))\n",
    "print('CV score:', gbc.score(X_cv, y_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 4 candidates, totalling 16 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  16 | elapsed: 21.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise-deprecating',\n",
       "       estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_sampl...      subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'learning_rate': [0.01, 0.05], 'max_depth': [8, 16], 'min_samples_leaf': [5], 'max_features': ['sqrt'], 'n_estimators': [300], 'random_state': [0]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if True:\n",
    "    grid = {\n",
    "        'learning_rate': [.01, .05],\n",
    "        'max_depth': [8, 16],\n",
    "        'min_samples_leaf': [5],\n",
    "        'max_features': ['sqrt'],\n",
    "        'n_estimators': [300],\n",
    "        'random_state': [0]\n",
    "    }\n",
    "else:  # TEST\n",
    "    grid = {\n",
    "    'learning_rate': [1],\n",
    "    'max_depth': [2], \n",
    "    'min_samples_leaf': [2],\n",
    "#     'max_features': ['sqrt', None],\n",
    "    'n_estimators': [2],\n",
    "    'random_state': [0]\n",
    "}\n",
    "    \n",
    "# confusion_score = make_scorer(confusion_rmse, greater_is_better=False)\n",
    "\n",
    "gbc_grid_cv = GridSearchCV(\n",
    "    GradientBoostingClassifier(), \n",
    "    grid,\n",
    "    cv=4,  # number of folds\n",
    "    return_train_score=True,\n",
    "    verbose=1, \n",
    "    n_jobs=-1)\n",
    "gbc_grid_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.05, 'max_depth': 16, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 300, 'random_state': 0}\n",
      "0.8815\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>param_min_samples_leaf</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>param_random_state</th>\n",
       "      <th>...</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>231.089302</td>\n",
       "      <td>5.176139</td>\n",
       "      <td>1.025364</td>\n",
       "      <td>0.063726</td>\n",
       "      <td>0.01</td>\n",
       "      <td>8</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>5</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8745</td>\n",
       "      <td>0.857500</td>\n",
       "      <td>0.012140</td>\n",
       "      <td>4</td>\n",
       "      <td>0.889333</td>\n",
       "      <td>0.890167</td>\n",
       "      <td>0.877833</td>\n",
       "      <td>0.889500</td>\n",
       "      <td>0.886708</td>\n",
       "      <td>0.005133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>359.524472</td>\n",
       "      <td>1.081115</td>\n",
       "      <td>1.536749</td>\n",
       "      <td>0.431736</td>\n",
       "      <td>0.01</td>\n",
       "      <td>16</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>5</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8770</td>\n",
       "      <td>0.865500</td>\n",
       "      <td>0.008958</td>\n",
       "      <td>3</td>\n",
       "      <td>0.904167</td>\n",
       "      <td>0.904000</td>\n",
       "      <td>0.898500</td>\n",
       "      <td>0.903000</td>\n",
       "      <td>0.902417</td>\n",
       "      <td>0.002305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>211.131388</td>\n",
       "      <td>3.241846</td>\n",
       "      <td>1.346371</td>\n",
       "      <td>0.529590</td>\n",
       "      <td>0.05</td>\n",
       "      <td>8</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>5</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8845</td>\n",
       "      <td>0.871875</td>\n",
       "      <td>0.010015</td>\n",
       "      <td>2</td>\n",
       "      <td>0.919333</td>\n",
       "      <td>0.920333</td>\n",
       "      <td>0.916333</td>\n",
       "      <td>0.918667</td>\n",
       "      <td>0.918667</td>\n",
       "      <td>0.001472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>340.674182</td>\n",
       "      <td>14.592542</td>\n",
       "      <td>0.754131</td>\n",
       "      <td>0.259608</td>\n",
       "      <td>0.05</td>\n",
       "      <td>16</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>5</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8880</td>\n",
       "      <td>0.881500</td>\n",
       "      <td>0.007492</td>\n",
       "      <td>1</td>\n",
       "      <td>0.953000</td>\n",
       "      <td>0.955333</td>\n",
       "      <td>0.952833</td>\n",
       "      <td>0.952333</td>\n",
       "      <td>0.953375</td>\n",
       "      <td>0.001157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0     231.089302      5.176139         1.025364        0.063726   \n",
       "1     359.524472      1.081115         1.536749        0.431736   \n",
       "2     211.131388      3.241846         1.346371        0.529590   \n",
       "3     340.674182     14.592542         0.754131        0.259608   \n",
       "\n",
       "  param_learning_rate param_max_depth param_max_features  \\\n",
       "0                0.01               8               sqrt   \n",
       "1                0.01              16               sqrt   \n",
       "2                0.05               8               sqrt   \n",
       "3                0.05              16               sqrt   \n",
       "\n",
       "  param_min_samples_leaf param_n_estimators param_random_state  ...  \\\n",
       "0                      5                300                  0  ...   \n",
       "1                      5                300                  0  ...   \n",
       "2                      5                300                  0  ...   \n",
       "3                      5                300                  0  ...   \n",
       "\n",
       "  split3_test_score  mean_test_score  std_test_score  rank_test_score  \\\n",
       "0            0.8745         0.857500        0.012140                4   \n",
       "1            0.8770         0.865500        0.008958                3   \n",
       "2            0.8845         0.871875        0.010015                2   \n",
       "3            0.8880         0.881500        0.007492                1   \n",
       "\n",
       "   split0_train_score  split1_train_score  split2_train_score  \\\n",
       "0            0.889333            0.890167            0.877833   \n",
       "1            0.904167            0.904000            0.898500   \n",
       "2            0.919333            0.920333            0.916333   \n",
       "3            0.953000            0.955333            0.952833   \n",
       "\n",
       "   split3_train_score  mean_train_score  std_train_score  \n",
       "0            0.889500          0.886708         0.005133  \n",
       "1            0.903000          0.902417         0.002305  \n",
       "2            0.918667          0.918667         0.001472  \n",
       "3            0.952333          0.953375         0.001157  \n",
       "\n",
       "[4 rows x 24 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = gbc_grid_cv.predict(X_cv)\n",
    "\n",
    "print(gbc_grid_cv.best_params_)\n",
    "print(gbc_grid_cv.best_score_)\n",
    "res_df = pd.DataFrame(gbc_grid_cv.cv_results_)\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "import pickle\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(model, open('GBC_'+ str(N_TREES) +'_' + str(LEARN_RATE) \n",
    "                        +'_' + str(MIN_IN_LEAF) + '_20min.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "### Use emotions vectors as input to model\n",
    "### Use emotions to inspect input and re-label if needed\n",
    "### See if remove highest frequency words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

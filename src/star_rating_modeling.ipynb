{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis to test:\n",
    "### Removing objective sentences from reviews helps predict star rating from reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import gzip\n",
    "import math\n",
    "import random\n",
    "from IPython.display import Markdown, display\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor,GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, make_scorer\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from utils import rmse, rmse_train_cv, classifier_report, confusion_rmse\n",
    "from to_bag_of_words import create_bag_of_words\n",
    "\n",
    "# Avoid restarting Kernel\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# %autosave 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsampling from Amazon reviews\n",
    "NB_SAMPLES = 20000  # up to 200k, then change the input file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get user comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../datasets/'\n",
    "file_name = '360000_balanced_train_test_reviews.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(data_path + file_name,\"rb\")\n",
    "train_test_dic = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create bag of words\n",
    "Remove accents  \n",
    "Tokenize  \n",
    "Lower the case\n",
    "Apply custom stop words (keep all negations)\n",
    " \n",
    "Output:  \n",
    "One list of words for each review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360000, 9)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_dic['train'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why would someone buy this pan & scan trash? do yourself a favor and recycle it!! The igonorance of some of these companies is overwhelming. This isn't the '80's any more, is it? Boycott rip-offs like this!\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_text = train_test_dic['train']['reviewText'][:3].values\n",
    "reviews_text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in',\n",
       " 'the',\n",
       " 'book',\n",
       " 'the',\n",
       " 'official',\n",
       " 'godzilla',\n",
       " 'compendium',\n",
       " ',',\n",
       " 'king',\n",
       " 'kong',\n",
       " 'vs',\n",
       " 'godzilla',\n",
       " 'is',\n",
       " 'described',\n",
       " 'as',\n",
       " '``',\n",
       " 'the',\n",
       " 'jaws',\n",
       " 'of',\n",
       " 'the',\n",
       " 'japanese',\n",
       " 'film',\n",
       " 'industry',\n",
       " \"''\",\n",
       " 'or',\n",
       " '``',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'great',\n",
       " 'monster',\n",
       " 'battles',\n",
       " 'of',\n",
       " 'cinema',\n",
       " 'history',\n",
       " \"''\",\n",
       " '.',\n",
       " 'it',\n",
       " 'is',\n",
       " 'referred',\n",
       " 'to',\n",
       " 'as',\n",
       " 'the',\n",
       " 'film',\n",
       " 'that',\n",
       " '``',\n",
       " '...',\n",
       " 'lifted',\n",
       " 'godzilla',\n",
       " 'from',\n",
       " 'the',\n",
       " 'swelling',\n",
       " 'ranks',\n",
       " 'of',\n",
       " 'interchangeable',\n",
       " 'atomic',\n",
       " 'monsters',\n",
       " 'of',\n",
       " 'the',\n",
       " 'fifties',\n",
       " 'and',\n",
       " 'placed',\n",
       " 'him',\n",
       " 'among',\n",
       " 'the',\n",
       " 'pantheon',\n",
       " 'of',\n",
       " 'cinema',\n",
       " 'creatures',\n",
       " \"''\",\n",
       " '.',\n",
       " 'thus',\n",
       " ',',\n",
       " 'i',\n",
       " 'had',\n",
       " 'high',\n",
       " 'expectations',\n",
       " 'when',\n",
       " 'i',\n",
       " 'watched',\n",
       " 'this',\n",
       " 'on',\n",
       " 'the',\n",
       " 'sci-fi',\n",
       " 'channel.all',\n",
       " 'i',\n",
       " 'have',\n",
       " 'to',\n",
       " 'say',\n",
       " 'is',\n",
       " ':',\n",
       " 'wow',\n",
       " '!',\n",
       " 'the',\n",
       " 'special',\n",
       " 'effects',\n",
       " 'are',\n",
       " 'incredible',\n",
       " '!',\n",
       " 'the',\n",
       " 'acting',\n",
       " 'is',\n",
       " 'top',\n",
       " 'notch',\n",
       " '.',\n",
       " 'the',\n",
       " 'screenplay',\n",
       " 'is',\n",
       " 'well',\n",
       " 'written',\n",
       " '.',\n",
       " 'this',\n",
       " 'film',\n",
       " 'is',\n",
       " 'truly',\n",
       " 'a',\n",
       " 'classic',\n",
       " 'of',\n",
       " 'cinema',\n",
       " 'that',\n",
       " 'will',\n",
       " 'leave',\n",
       " 'you',\n",
       " 'thinking',\n",
       " 'long',\n",
       " 'after',\n",
       " 'you',\n",
       " 'leave',\n",
       " 'the',\n",
       " 'theater.wait',\n",
       " '.',\n",
       " 'that',\n",
       " \"'s\",\n",
       " 'my',\n",
       " 'review',\n",
       " 'for',\n",
       " '2001',\n",
       " ':',\n",
       " 'a',\n",
       " 'space',\n",
       " 'odyssey.here',\n",
       " \"'s\",\n",
       " 'what',\n",
       " 'i',\n",
       " 'really',\n",
       " 'think',\n",
       " 'of',\n",
       " 'king',\n",
       " 'kong',\n",
       " 'vs.',\n",
       " 'godzilla.a',\n",
       " 'thousand',\n",
       " 'words',\n",
       " 'can',\n",
       " 'not',\n",
       " 'describe',\n",
       " 'my',\n",
       " 'dislike',\n",
       " 'of',\n",
       " 'the',\n",
       " 'film',\n",
       " '.',\n",
       " 'i',\n",
       " 'know',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'a',\n",
       " 'godzilla',\n",
       " 'film',\n",
       " ',',\n",
       " 'but',\n",
       " 'did',\n",
       " 'it',\n",
       " 'need',\n",
       " 'to',\n",
       " 'be',\n",
       " 'this',\n",
       " 'bad',\n",
       " '?',\n",
       " 'it',\n",
       " 'starts',\n",
       " 'off',\n",
       " 'good',\n",
       " ',',\n",
       " 'with',\n",
       " 'an',\n",
       " 'excellent',\n",
       " 'piece',\n",
       " 'of',\n",
       " 'music',\n",
       " 'at',\n",
       " 'the',\n",
       " 'beginning',\n",
       " 'credits',\n",
       " '.',\n",
       " 'but',\n",
       " 'soon',\n",
       " 'goes',\n",
       " 'downhill',\n",
       " 'as',\n",
       " 'we',\n",
       " 'are',\n",
       " 'introduced',\n",
       " 'to',\n",
       " 'u.n',\n",
       " 'reporter',\n",
       " 'eric',\n",
       " 'carter',\n",
       " '.',\n",
       " 'all',\n",
       " 'he',\n",
       " 'was',\n",
       " 'put',\n",
       " 'in',\n",
       " 'for',\n",
       " 'was',\n",
       " 'to',\n",
       " 'make',\n",
       " 'the',\n",
       " 'film',\n",
       " 'longer',\n",
       " 'and',\n",
       " 'annoy',\n",
       " 'us',\n",
       " '.',\n",
       " 'he',\n",
       " '``',\n",
       " 'communicates',\n",
       " \"''\",\n",
       " 'with',\n",
       " 'people',\n",
       " 'in',\n",
       " 'japan',\n",
       " 'via',\n",
       " 'a',\n",
       " 'satellite',\n",
       " 'that',\n",
       " 'looks',\n",
       " 'more',\n",
       " 'like',\n",
       " 'a',\n",
       " 'first',\n",
       " 'grade',\n",
       " 'science',\n",
       " 'project',\n",
       " 'than',\n",
       " 'visual',\n",
       " 'effects',\n",
       " '.',\n",
       " 'he',\n",
       " 'constantly',\n",
       " 'tells',\n",
       " 'us',\n",
       " 'that',\n",
       " '``',\n",
       " 'godzilla',\n",
       " 'is',\n",
       " 'heading',\n",
       " 'for',\n",
       " 'japan',\n",
       " \"''\",\n",
       " 'or',\n",
       " '``',\n",
       " 'king',\n",
       " 'kong',\n",
       " 'and',\n",
       " 'godzilla',\n",
       " 'will',\n",
       " 'battle',\n",
       " '.',\n",
       " \"''\",\n",
       " 'duh',\n",
       " '!',\n",
       " 'what',\n",
       " 'do',\n",
       " 'you',\n",
       " 'think',\n",
       " 'king',\n",
       " 'kong',\n",
       " 'and',\n",
       " 'godzilla',\n",
       " 'will',\n",
       " 'do',\n",
       " '?',\n",
       " 'have',\n",
       " 'tea',\n",
       " 'and',\n",
       " 'crumpets',\n",
       " '?',\n",
       " 'speaking',\n",
       " 'of',\n",
       " 'kong',\n",
       " ',',\n",
       " 'i',\n",
       " 'have',\n",
       " 'seen',\n",
       " 'costumes',\n",
       " 'that',\n",
       " 'look',\n",
       " 'better',\n",
       " 'than',\n",
       " 'this',\n",
       " 'kong',\n",
       " 'suit',\n",
       " '.',\n",
       " 'i',\n",
       " 'would',\n",
       " 'also',\n",
       " 'rather',\n",
       " 'listen',\n",
       " 'to',\n",
       " 'celine',\n",
       " 'dion',\n",
       " 'and',\n",
       " 'elton',\n",
       " 'john',\n",
       " 'do',\n",
       " 'a',\n",
       " 'music',\n",
       " 'number',\n",
       " 'together',\n",
       " 'than',\n",
       " 'listen',\n",
       " 'to',\n",
       " 'kong',\n",
       " 'roar',\n",
       " '.',\n",
       " 'he',\n",
       " 'does',\n",
       " \"n't\",\n",
       " 'even',\n",
       " 'open',\n",
       " 'his',\n",
       " 'mouth',\n",
       " '!',\n",
       " 'godzilla',\n",
       " 'appears',\n",
       " 'just',\n",
       " 'fine',\n",
       " ',',\n",
       " 'except',\n",
       " 'for',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'he',\n",
       " \"'s\",\n",
       " 'afraid',\n",
       " 'of',\n",
       " 'thermal',\n",
       " 'energy',\n",
       " '(',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'he',\n",
       " 'tear',\n",
       " 'down',\n",
       " 'an',\n",
       " 'electrical',\n",
       " 'fence',\n",
       " 'in',\n",
       " '1954',\n",
       " 'with',\n",
       " 'ease',\n",
       " '?',\n",
       " ')',\n",
       " '.',\n",
       " 'kong',\n",
       " 'however',\n",
       " 'gets',\n",
       " 'power',\n",
       " 'from',\n",
       " 'the',\n",
       " 'thermal',\n",
       " 'energy',\n",
       " '.',\n",
       " 'gee',\n",
       " ',',\n",
       " 'i',\n",
       " 'would',\n",
       " 'assume',\n",
       " 'that',\n",
       " 'all',\n",
       " 'his',\n",
       " 'fur',\n",
       " 'would',\n",
       " 'make',\n",
       " 'him',\n",
       " 'the',\n",
       " 'perfect',\n",
       " 'conductor',\n",
       " 'of',\n",
       " 'elctricity',\n",
       " '.',\n",
       " 'also',\n",
       " ',',\n",
       " 'does',\n",
       " \"n't\",\n",
       " 'anyone',\n",
       " 'else',\n",
       " 'notice',\n",
       " 'that',\n",
       " 'the',\n",
       " 'islands',\n",
       " 'of',\n",
       " 'the',\n",
       " 'first',\n",
       " 'godzilla',\n",
       " 'movies',\n",
       " '(',\n",
       " 'the',\n",
       " \"'50s\",\n",
       " 'to',\n",
       " 'the',\n",
       " \"'70s\",\n",
       " ')',\n",
       " 'always',\n",
       " 'look',\n",
       " 'the',\n",
       " 'same',\n",
       " '?',\n",
       " 'the',\n",
       " 'main',\n",
       " 'battles',\n",
       " 'themselves',\n",
       " 'are',\n",
       " 'also',\n",
       " 'letdowns.it',\n",
       " 'was',\n",
       " 'a',\n",
       " 'grand',\n",
       " 'idea',\n",
       " 'that',\n",
       " 'toho',\n",
       " 'had',\n",
       " ':',\n",
       " 'pitting',\n",
       " 'two',\n",
       " 'titans',\n",
       " 'of',\n",
       " 'cinema',\n",
       " 'against',\n",
       " 'each',\n",
       " 'other',\n",
       " '.',\n",
       " 'too',\n",
       " 'bad',\n",
       " 'the',\n",
       " 'japanese',\n",
       " 'did',\n",
       " 'not',\n",
       " 'use',\n",
       " 'stop',\n",
       " 'motion',\n",
       " 'animation',\n",
       " '.',\n",
       " 'or',\n",
       " 'make',\n",
       " 'a',\n",
       " 'kong/godzilla',\n",
       " 'rematch',\n",
       " '.',\n",
       " 'or',\n",
       " 'make',\n",
       " 'better',\n",
       " 'costumes',\n",
       " 'or',\n",
       " 'etc',\n",
       " ',',\n",
       " 'etc',\n",
       " '.',\n",
       " 'for',\n",
       " 'a',\n",
       " 'true',\n",
       " 'classic',\n",
       " 'of',\n",
       " 'monster',\n",
       " 'cinema',\n",
       " ',',\n",
       " 'king',\n",
       " 'kong',\n",
       " ',',\n",
       " 'the',\n",
       " 'original',\n",
       " 'godzilla',\n",
       " 'or',\n",
       " 'godzilla',\n",
       " '1985',\n",
       " 'are',\n",
       " 'better',\n",
       " 'choices',\n",
       " 'than',\n",
       " 'this',\n",
       " 'pile',\n",
       " 'of',\n",
       " '...',\n",
       " 'if',\n",
       " 'this',\n",
       " 'was',\n",
       " 'the',\n",
       " 'film',\n",
       " 'that',\n",
       " 'made',\n",
       " 'godzilla',\n",
       " 'famous',\n",
       " ',',\n",
       " 'he',\n",
       " 'should',\n",
       " 'have',\n",
       " 'been',\n",
       " 'assassinated.indeed',\n",
       " 'this',\n",
       " 'film',\n",
       " 'will',\n",
       " 'leave',\n",
       " 'you',\n",
       " 'thinking',\n",
       " 'after',\n",
       " 'you',\n",
       " 'leave',\n",
       " 'the',\n",
       " 'theater',\n",
       " ':',\n",
       " 'why',\n",
       " 'did',\n",
       " 'i',\n",
       " 'waste',\n",
       " 'my',\n",
       " 'time',\n",
       " 'on',\n",
       " 'this',\n",
       " '?',\n",
       " '(',\n",
       " 'note',\n",
       " ':',\n",
       " 'perhaps',\n",
       " 'my',\n",
       " 'criticism',\n",
       " 'was',\n",
       " 'a',\n",
       " 'bit',\n",
       " 'harsh',\n",
       " '.',\n",
       " 'after',\n",
       " 'all',\n",
       " ',',\n",
       " 'this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'american',\n",
       " 'version',\n",
       " 'of',\n",
       " 'the',\n",
       " 'movie',\n",
       " 'and',\n",
       " 'we',\n",
       " 'always',\n",
       " '[',\n",
       " 'mess',\n",
       " ']',\n",
       " 'with',\n",
       " 'perfection',\n",
       " '.',\n",
       " 'maybe',\n",
       " 'the',\n",
       " 'japanese',\n",
       " 'version',\n",
       " 'is',\n",
       " 'better',\n",
       " '.',\n",
       " 'get',\n",
       " 'that',\n",
       " 'one',\n",
       " 'if',\n",
       " 'you',\n",
       " 'can',\n",
       " ')',\n",
       " '.']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "res = create_bag_of_words(reviews_text)\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "{'then', \"you're\", 'she', 'few', 'yourself', 'or', 'itself', 'does', 'out', 'isn', 'until', 'themselves', 'you', 'y', 'ain', 'wouldn', 'because', \"won't\", 'mustn', 'of', 'on', 'not', 'very', 'is', \"doesn't\", \"weren't\", 'below', \"mightn't\", 'further', 'himself', 'myself', \"didn't\", 'so', 'only', 'll', 'my', \"don't\", 'same', 'am', 'her', 'but', 'had', 'a', \"hasn't\", 'where', 'aren', 'other', 'be', \"shan't\", 'him', \"you've\", 'own', 'the', \"isn't\", 'are', 'were', 'haven', 'this', \"couldn't\", 'at', 'our', 'from', 'doesn', 'if', 'and', 'm', 'before', 'off', 've', 'by', \"you'd\", 'after', 'who', 'has', 'some', 'those', 'as', 'about', 'no', 'me', 'its', \"you'll\", 'have', 'through', \"wouldn't\", 'over', 'too', 'such', 'yourselves', 'he', 'his', 'them', \"aren't\", 'under', 'they', 'again', 'above', 'herself', 'between', \"it's\", 'don', 're', 'against', 'all', 'whom', \"should've\", 'during', 'here', 'now', 'do', 'to', 'can', 'up', 'ma', 'it', 'shan', 'theirs', 'we', 'what', 'yours', 'o', 'hadn', 'shouldn', 'hasn', 'wasn', 'which', 'once', 'why', 'doing', 'weren', 'when', 'been', 'with', 'hers', 'just', 'will', 'needn', \"she's\", 'won', 'for', 'in', \"haven't\", 'more', 'their', 'most', 's', 'nor', 'these', 'an', \"mustn't\", 'down', 'ours', 'into', 'd', 'having', 'both', 'each', 'your', 'did', 'how', \"shouldn't\", 'there', 'that', 'while', 'than', \"hadn't\", 'didn', \"wasn't\", 'was', 't', 'should', 'couldn', 'mightn', 'i', \"that'll\", 'any', 'being', \"needn't\", 'ourselves'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_ = set(stopwords.words('english'))\n",
    "print(len(stopwords_))\n",
    "print(stopwords_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Remove reviews that may not be on the movie, but on Amazon/support instead\n",
    "Input: \n",
    "* word tokens \n",
    "* one line per review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: merge negations with next word, remove next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: encode the review length as other input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Star rating modeling for base case, based on tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_cv, y_train, y_cv = train_test_split(\n",
    "  balanced_reviews['reviewText'], balanced_reviews['overall'], test_size=0.2, random_state=0)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(lowercase=True, \n",
    "                        stop_words='english', \n",
    "                        max_features=MAX_FEATURES,\n",
    "                        norm='l2',            # normalize each review\n",
    "                        use_idf=True)        # Keep high weight for most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "(5000, 20000)\n"
     ]
    }
   ],
   "source": [
    "X_train_tf = tfidf.fit_transform(X_train)\n",
    "X_cv_tf = tfidf.transform(X_cv)\n",
    "print(X_cv_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    }
   ],
   "source": [
    "pickle_out = open(\"pickles/tfidf_25kBalancedSamples_20kFeats.pkl\",\"wb\")\n",
    "pickle.dump(tfidf, pickle_out)\n",
    "pickle_out.close()\n",
    "print(len(tfidf.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Classifier parameters\n",
    "N_TREES = 300 # math.floor(np.sqrt(NB_SAMPLES) * 1.2)\n",
    "LEARN_RATE = 0.1\n",
    "MAX_DEPTH = 15\n",
    "MIN_IN_LEAF = 2 #7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier(learning_rate=LEARN_RATE, \n",
    "                                n_estimators=N_TREES, \n",
    "                                min_samples_leaf=MIN_IN_LEAF,\n",
    "                                max_depth=MAX_DEPTH,\n",
    "A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=15,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=2, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=300,\n",
       "              n_iter_no_change=None, presort='auto', random_state=0,\n",
       "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc.fit(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(gbc, open('pickles/GBC_after_tfidf_balanced_comments_'\n",
    "                       + str(N_TREES) + '_trees_' \n",
    "                       + str(LEARN_RATE) + '_lr_' \n",
    "                       + str(MAX_DEPTH) + '_maxdpth_'\n",
    "                       + str(MIN_IN_LEAF) + '_minleaf_'\n",
    "                       + str(MAX_FEATURES) + '_feats_'\n",
    "                       + '.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000  features 300 trees;  0.1 learn_rate;  15 max_dpth;  2 min_in_leaf\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Report for Gradient Boosting Classifier on training set:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Confusion RMSE: 0.101"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Confusion Matrix:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4017    0    2    0    0]\n",
      " [   1 3964    8    4    3]\n",
      " [   8    4 3995   10   16]\n",
      " [   1    2    3 3972    4]\n",
      " [   0    0    3    0 3983]]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Classification Report:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      1.00      1.00      4019\n",
      "         2.0       1.00      1.00      1.00      3980\n",
      "         3.0       1.00      0.99      0.99      4033\n",
      "         4.0       1.00      1.00      1.00      3982\n",
      "         5.0       0.99      1.00      1.00      3986\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     20000\n",
      "   macro avg       1.00      1.00      1.00     20000\n",
      "weighted avg       1.00      1.00      1.00     20000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Report for Gradient Boosting Classifier on CV set:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Confusion RMSE: 1.305"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Confusion Matrix:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[513 230 123  63  52]\n",
      " [224 339 270 121  66]\n",
      " [ 94 199 339 216 119]\n",
      " [ 62 110 282 326 238]\n",
      " [ 45  56 144 233 536]]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Classification Report:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.55      0.52      0.53       981\n",
      "         2.0       0.36      0.33      0.35      1020\n",
      "         3.0       0.29      0.35      0.32       967\n",
      "         4.0       0.34      0.32      0.33      1018\n",
      "         5.0       0.53      0.53      0.53      1014\n",
      "\n",
      "   micro avg       0.41      0.41      0.41      5000\n",
      "   macro avg       0.41      0.41      0.41      5000\n",
      "weighted avg       0.41      0.41      0.41      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(MAX_FEATURES, ' features', N_TREES,'trees; ',\n",
    "      LEARN_RATE,'learn_rate; ', MAX_DEPTH, 'max_dpth; ',\n",
    "      MIN_IN_LEAF, 'min_in_leaf')\n",
    "classifier_report(gbc, X_train_tf, y_train,\n",
    "                  'Gradient Boosting Classifier on training set')\n",
    "classifier_report(gbc, X_cv_tf, y_cv, \n",
    "                  'Gradient Boosting Classifier on CV set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    gb_pipe = Pipeline([('vect', tfidf), ('gb', gbc)])\n",
    "    gb_pipe.fit(X_train, y_train)\n",
    "    pickle.dump(gb_pipe, open('pickles/GBCpipe_balanced_comments_'\n",
    "                           + str(N_TREES) + '_trees_' \n",
    "                           + str(LEARN_RATE) + '_lr_' \n",
    "                           + str(MAX_DEPTH) + '_maxdpth_'\n",
    "                           + str(MIN_IN_LEAF) + '_minleaf_'\n",
    "                           + str(MAX_FEATURES) + '_feats_'\n",
    "                           + '.pkl', 'wb'))\n",
    "else:\n",
    "    pickle_in = open(\"pickles/GBC_balanced_comments_300_trees_0.1_lr_15_maxdpth_2_minleaf_20000_feats_.pkl\",\n",
    "                     \"rb\")\n",
    "    gb_pipe = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000  features 300 trees;  0.1 learn_rate;  15 max_dpth;  2 min_in_leaf\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Report for Gradient Boosting Classifier on training set:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Confusion RMSE: 0.101"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Off diagonal: 0.00"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Confusion Matrix:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4017    0    2    0    0]\n",
      " [   1 3964    8    4    3]\n",
      " [   8    4 3995   10   16]\n",
      " [   1    2    3 3972    4]\n",
      " [   0    0    3    0 3983]]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Classification Report:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      1.00      1.00      4019\n",
      "         2.0       1.00      1.00      1.00      3980\n",
      "         3.0       1.00      0.99      0.99      4033\n",
      "         4.0       1.00      1.00      1.00      3982\n",
      "         5.0       0.99      1.00      1.00      3986\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     20000\n",
      "   macro avg       1.00      1.00      1.00     20000\n",
      "weighted avg       1.00      1.00      1.00     20000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Report for Gradient Boosting Classifier on CV set:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Confusion RMSE: 1.305"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Off diagonal: 0.59"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Confusion Matrix:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[513 230 123  63  52]\n",
      " [224 339 270 121  66]\n",
      " [ 94 199 339 216 119]\n",
      " [ 62 110 282 326 238]\n",
      " [ 45  56 144 233 536]]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Classification Report:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.55      0.52      0.53       981\n",
      "         2.0       0.36      0.33      0.35      1020\n",
      "         3.0       0.29      0.35      0.32       967\n",
      "         4.0       0.34      0.32      0.33      1018\n",
      "         5.0       0.53      0.53      0.53      1014\n",
      "\n",
      "   micro avg       0.41      0.41      0.41      5000\n",
      "   macro avg       0.41      0.41      0.41      5000\n",
      "weighted avg       0.41      0.41      0.41      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "print(MAX_FEATURES, ' features', N_TREES,'trees; ',\n",
    "      LEARN_RATE,'learn_rate; ', MAX_DEPTH, 'max_dpth; ',\n",
    "      MIN_IN_LEAF, 'min_in_leaf')\n",
    "classifier_report(gb_pipe, X_train, y_train,\n",
    "                  'Gradient Boosting Classifier on training set')\n",
    "classifier_report(gb_pipe, X_cv, y_cv, \n",
    "                  'Gradient Boosting Classifier on CV set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misclassified CV samples: 5 stars instead of 1 star\n",
    "Some words are very positive, the negativity is more subtle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = gb_pipe.predict(X_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([False,  True]), array([4948,   52]))\n",
      "5000\n",
      "5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "149193    I did not know one could write, produce and especially direct a film like this one. The accumulation of unbearable (I hope this film does not reflect the average American suburban precollege teen) nonsense is hardly  imaginable. I am really sorry about those people who liked this film and  especially two things:  a. the dichotomy between explicit verbal sex and  the absolute absence of any realistic love scene (I would be very  astonished if every single American teen is negotiating his First Time like  this).  b. the stereotypical image of the east-European girl. Although  Nadia was really amazing (but a bit too solarium burnt and silicon breasted  for being a Check or Slovakian girl (note for the scriptwriter: there is no  Checkoslovakia any more), I wonder why she was the only one who had to get  naked (and no American girl or guy)!  Well that's it. Thanks for reading  and sorry for those who might like the film.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "89223     The product results in John Wayne playing the same role he always played, just in a different setting, and the script not being true to life. The reality of the battle is not there - the movie is mostly fantasy peppered with a few facts to give it some credibility. How do I know it was not true to life? My father was there. He was a John Wayne fan but was disgusted by the movie.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "23263     One of my favorite movies and the sound is off, not in sync with their speakingl  Very dissatisfied.  Ordered this product and was very dissappointed.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "18318     I love this film, but Amazon includes reviews from different versions of the film on this one's product page. This is a FULL-SCREEN version of the film -- in other words, half of the movie has been cut off. If you're hoping to see the whole movie, search for &#34;Leon - The Professional (Uncut International Version)&#34;.That said, Amazon was great at allowing me to return this junk, and instantly credited my account...so I immediately bought the real thing.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "85029     love the movie but it never showed up the package delv guys said that they tried but they did not                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "64934     Although the movie was shot beautifully, it was too simplistic and dull. It contained a plot with nothing more than just two childish people playing games with each other in a very uninteresting relationship. The supporting  actors, who seemed more interesting than Nia and Larenz characters, were  wasted and might as well have been cast as extras since thier appearences  were so brief. Not that it would help this picture much, but as much as I  love Larenz Tate, that role was crying for someone who can carry on as a  romantic leading man such as Mikhal Phiffer *I hope I spelled that name  right* Like my caption says, skip this video and PICK UP THAT SOUNDTRACK!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "57995     I never received my movie, so how can I give it a rating?  This is the first time that this has happened to an order that I have placed.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "119279    see message above. please email when completed. the movie was purchased on the television and failed to download on said television.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "122219    This movie makes its point, but it makes men look like pigs, and I do not classify myself amongst them.  I am well aware of what Clinton and all politicians are and do.  I think he did some excellent things as president even though I am republican.  But I still found it disappointing and I do not understand why all of you think it is so fabulous.  If you are following the news and such, what do you need to watch this for anyway?  Travolta created an excellent portrayal, and it exposed Clinton for what he really is, despite the few things he did do correctly. I am surprised today's audience is so easily swayed by a movie, and this in nothing more than entertainment far from its best. I understand most do not agree with me, but I do not care, nor is it about to alter my opinion.  There are a lot better ways to spend your time time than watching this, and I happen to posses  Ph.D from Claremont College, CA, and am 60 years old.  I am not afraid to say I have old fashioned values such as respect for women and not admiring the idolization of a president who should have very well been impeached, when he was not playing with his manhood for the benefit of the public. I think he set a trend; expose your manhood and your dirty secrets - it's okay with all of us. No, I am sorry; it is not.  I have class and an education.  This does nothave class, nor does he. It is an embarassment to our country, as is our current \"leader\", loosely termed, no pun intended.  Small minds.  It is a movie celebrating someone who should have been impeached, the epitome of immorality and deceit. No wonder our country is where it is today.  We have fools like this, that many of us seem to idolize, to thank for it.\n",
       "69946     the movie is great, but I paid for the HD rental and I was getting like 240p quality from amazon. Also this is most definitely amazon's fault because I ran a few connection tests to see if it was my ISP.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       "Name: reviewText, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.set_printoptions(threshold=sys.maxsize)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "true_1_pred_5 = (y_predict == 5) & (y_cv == 1)\n",
    "print(np.unique(true_1_pred_5, return_counts=True))\n",
    "print(len(true_1_pred_5))\n",
    "print(len(X_cv))\n",
    "X_cv[true_1_pred_5][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other models in star_rating_modeling2 !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split comments into separate sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "small['sentence'] = small['reviewText'].map(sent_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small.drop(['reviewerName', 'helpful', 'reviewText', 'summary', \n",
    "            'unixReviewTime', 'reviewTime'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = small['sentence'] \\\n",
    ".apply(pd.Series) \\\n",
    ".merge(small, left_index = True, right_index = True) \\\n",
    ".drop(['sentence'], axis = 1) \\\n",
    ".melt(id_vars = ['reviewerID', 'asin','overall'], value_name = 'sentence') \\\n",
    ".drop(['variable'], axis = 1) \\\n",
    ".dropna()\n",
    "\n",
    "print(sentences.shape)\n",
    "sentences.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence-level prep & cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "from utils import split_n_lower, not_about_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into words and lower the case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences['words'] = sentences['sentence'].apply(lambda s: split_n_lower(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences.shape)\n",
    "sentences.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep support-related sentences as they probably have impact on rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on_movies_filter = [not_about_support(word) for word in sentences['words']]\n",
    "sentences_on_movie = sentences #[on_movies_filter]\n",
    "\n",
    "print('Removing {} records'.format(sentences.shape[0]- sentences_on_movie.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_on_movie.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base case: A reviews with objective and subjective sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel dies here at 50K samples\n",
    "all_reviews_groups = sentences_on_movie.groupby(['reviewerID','asin'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews_stars = all_reviews_groups['overall'].mean()\n",
    "all_reviews_stars[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews_comments = all_reviews_groups['words'].sum()\n",
    "print(sentences_on_movie.iloc[0, 4])\n",
    "print(all_reviews_comments.shape)\n",
    "print(all_reviews_comments[0])\n",
    "len(all_reviews_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove objective sentences for case B using obj-subj model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_colwidth', -1)\n",
    "# sentences_on_movie['sentence']\n",
    "sentences_on_movie.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_on_movie.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize along the word space of the obj-subj training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = pickle.load(open('pickles/Obj-Subj_tfidf.pkl', 'rb'))\n",
    "len(tfidf.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_tfidf = tfidf.transform(sentences_on_movie['sentence']).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply the obj-subj model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TREES = 100\n",
    "LEARN_RATE = 0.1\n",
    "MIN_IN_LEAF = 10\n",
    "pickle_in = open('pickles/GBC_'+ str(N_TREES) +'_' + str(LEARN_RATE) \n",
    "                        +'_' + str(MIN_IN_LEAF) + '_20min.pkl', 'rb')\n",
    "gb_model = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = gb_model.predict(sentences_tfidf)\n",
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjective_sentences = sentences_on_movie[y_test == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown('### Removing {} objective sentences'\n",
    "                 .format(len(y_test) - len(subjective_sentences))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjective_sentences.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge the sentences back into paragraph reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_groups = subjective_sentences.groupby(['reviewerID','asin'])\n",
    "subj_reviews_stars = subj_groups['overall'].mean()\n",
    "# subjective_reviewssubjective_reviews['sentence'].apply(lambda x: x.sum())\n",
    "# subjective_reviews_reviews = \n",
    "subj_reviews_stars[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_review_comments = subj_groups['words'].sum()\n",
    "print(subj_review_comments.shape)\n",
    "print(subj_review_comments[0])\n",
    "subj_review_comments[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that stars still correspond to the right movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 6000\n",
    "end = 6010\n",
    "all_reviews_comments.loc[('A33Z7JTV7SSW9Y', '0718000315')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_reviews_stars.loc[('A33Z7JTV7SSW9Y', '0718000315')])\n",
    "print(sentences_on_movie.loc[sentences_on_movie['reviewerID']=='A33Z7JTV7SSW9Y']) \n",
    "# and sentences_on_movie['asin']=='0718000315'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = -1\n",
    "print(small.loc[small['reviewerID']=='A33Z7JTV7SSW9Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_on_movie[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create emotion vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of reviews:', all_reviews_comments.shape[0])\n",
    "print('Total number of subjective reviews:', subj_review_comments.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "from emotions_seven import Emotions7\n",
    "emote = Emotions7()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews_emotions = emote.vectorize(all_reviews_comments)\n",
    "print(all_reviews_emotions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emote.emotions_in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews_emotions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_revs_with_emotions = all_reviews_emotions[emote.emotions_in_text == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(all_revs_with_emotions.shape)\n",
    "# all_revs_stars = all_reviews_stars[emote.emotions_in_text]\n",
    "all_reviews_emotions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_reviews_emotions = emote.vectorize(subj_review_comments)\n",
    "print(subj_reviews_emotions.shape)\n",
    "subj_reviews_emotions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a model on base case (all comments) for star rating prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_subj_train, X_subj_cv, y_subj_train, y_subj_cv = train_test_split(\n",
    "    subj_reviews_emotions, subj_reviews_stars, test_size=0.2, random_state=0)\n",
    "X_subj_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_subj = GradientBoostingClassifier(learning_rate=LEARN_RATE, \n",
    "                                n_estimators=N_TREES, \n",
    "                                min_samples_leaf=MIN_IN_LEAF,\n",
    "                                random_state=0)\n",
    "gbc_subj.fit(X_subj_train, y_subj_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Gradient Boosting Classifier')\n",
    "print('Training score using all comments: {0:.2f}'\n",
    "      .format(gbc_all.score(X_train, y_train)))\n",
    "print('CV score using all comments: {0:.2f}'\n",
    "      .format(gbc_all.score(X_cv, y_cv)))\n",
    "print('')\n",
    "\n",
    "# print('Training score using subjective comments only: {0:.2f}'\n",
    "#       .format(gbc_subj.score(X_subj_train, y_subj_train)))\n",
    "# print('CV score using subjective comments only: {0:.2f}'\n",
    "#       .format(gbc_subj.score(X_subj_cv, y_subj_cv)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# lr = LogisticRegression(random_state=0, solver='lbfgs',\n",
    "#                        multi_class='multinomial',max_iter=1000)\n",
    "# lr.fit(X_subj_train, y_subj_train)\n",
    "# print(lr.score(X_subj_train, y_subj_train))\n",
    "# print(lr.score(X_subj_cv, y_subj_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_all = sm.OLS(y_train, X_train)\n",
    "results_all = ols_all.fit()\n",
    "results_all.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ols_subj = sm.OLS(y_subj_train, X_subj_train)\n",
    "results_subj = ols_subj.fit()\n",
    "results_subj.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "all_reviews_emotions, all_reviews_stars\n",
    "\n",
    "sns.heatmap(raw_df.corr(), annot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

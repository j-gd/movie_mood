{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis to test:\n",
    "### Removing objective sentences from reviews helps predict star rating from reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import gzip\n",
    "import math\n",
    "import random\n",
    "from IPython.display import Markdown, display\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor, \\\n",
    "GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, \\\n",
    "classification_report, make_scorer\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# From this project\n",
    "from utils import rmse, rmse_train_cv, classifier_report, confusion_rmse\n",
    "from NLP import WordBag, AboutMovie\n",
    "from subjective_filter import SubjectiveFilter\n",
    "\n",
    "# Avoid restarting Kernel\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "# %autosave 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsampling from Amazon reviews\n",
    "NB_SAMPLES = 360000 #4000  # up to 200k, then change the input file\n",
    "\n",
    "data_path = '../../../datasets/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get users' positive and negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name = '360000_balanced_train_test_reviews.pkl'\n",
    "file_name = '_balanced_pos_neg_train_test_reviews.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(data_path + str(NB_SAMPLES) + file_name,\"rb\")\n",
    "train_test_dic0 = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_FRACTION = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_dic = {'train': {}, 'test':{}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['train','test']:\n",
    "    for j in ['positive','negative']:\n",
    "         train_test_dic[i][j] = train_test_dic0[i][j] \\\n",
    "            .iloc[:math.floor(len(train_test_dic0[i][j].index) * SAMPLE_FRACTION), :] \\\n",
    "            .drop(['reviewerName', 'helpful', 'summary', 'unixReviewTime', 'reviewTime'], axis=1)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train-CV and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = {}\n",
    "y = {}\n",
    "for i in ['train','test']:\n",
    "    reviews[i] = pd.concat([train_test_dic[i]['positive'],\n",
    "                     train_test_dic[i]['negative']]).reset_index(drop=True)\n",
    "    y[i] = np.concatenate([np.ones((train_test_dic[i]['positive'].shape[0],)), \n",
    "                          np.zeros((train_test_dic[i]['negative'].shape[0],))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info(dic, nb_lines=2):\n",
    "    for i in ['train','test']:\n",
    "        display(Markdown('#### {}:'.format(i)))\n",
    "        print(dic[i].shape)\n",
    "        display(dic[i].head(nb_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### train:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2880, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A32244V7CQUBD6</td>\n",
       "      <td>B00005QFEK</td>\n",
       "      <td>This video actually focuses mostly on one of the characters that Emmanuelle (Krista Allen) is trying to teach about sex &amp; love. It's still pretty entertaining but if you are mostly interested in Kirsta Allen then you should know that she's not really in much of this episode.</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A32244V7CQUBD6</td>\n",
       "      <td>B00005QFEN</td>\n",
       "      <td>This episode pretty much has Hafron and Emmanuelle teleporting to different parts of the world and &amp;quot;doing it&amp;quot;. There is the continuing plot from an earlier episode of some group on Earth trying to track them down. That's the main reason for Emmanuelle and Hafron to jump to different parts of the world. Otherwise, this episode is mostly sex scenes.</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin  \\\n",
       "0  A32244V7CQUBD6  B00005QFEK   \n",
       "1  A32244V7CQUBD6  B00005QFEN   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                reviewText  \\\n",
       "0  This video actually focuses mostly on one of the characters that Emmanuelle (Krista Allen) is trying to teach about sex & love. It's still pretty entertaining but if you are mostly interested in Kirsta Allen then you should know that she's not really in much of this episode.                                                                                       \n",
       "1  This episode pretty much has Hafron and Emmanuelle teleporting to different parts of the world and &quot;doing it&quot;. There is the continuing plot from an earlier episode of some group on Earth trying to track them down. That's the main reason for Emmanuelle and Hafron to jump to different parts of the world. Otherwise, this episode is mostly sex scenes.   \n",
       "\n",
       "   overall  \n",
       "0  4.0      \n",
       "1  4.0      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### test:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A2D832OA6Q5ZAS</td>\n",
       "      <td>B00005QFFP</td>\n",
       "      <td>What a pleasure to see this peerless diva perform -- she is the purest example of her art that I have ever known. Like a great actress, she dissolves into her role, and yet her voice and style are easily recognizable for the effortless simplicity with which she nails every note and figure. I wish they could have taped in color in those days, and I would have liked to see her perform more examples of the coloratura repertoire and less of the popular romantic themes.  Overall, I am delighted with this video.</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1JF78EP4GPAOO</td>\n",
       "      <td>B00005QG2N</td>\n",
       "      <td>The concert is fantastic as are the videos and Cradle of Fear trailer. The rest however seems to be uninspired filler. I would have liked this better if it included the Pandaemonaeon music videos and Her Ghost in the Fog. Where was the BBC documentary?</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin  \\\n",
       "0  A2D832OA6Q5ZAS  B00005QFFP   \n",
       "1  A1JF78EP4GPAOO  B00005QG2N   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        reviewText  \\\n",
       "0  What a pleasure to see this peerless diva perform -- she is the purest example of her art that I have ever known. Like a great actress, she dissolves into her role, and yet her voice and style are easily recognizable for the effortless simplicity with which she nails every note and figure. I wish they could have taped in color in those days, and I would have liked to see her perform more examples of the coloratura repertoire and less of the popular romantic themes.  Overall, I am delighted with this video.   \n",
       "1  The concert is fantastic as are the videos and Cradle of Fear trailer. The rest however seems to be uninspired filler. I would have liked this better if it included the Pandaemonaeon music videos and Her Ghost in the Fog. Where was the BBC documentary?                                                                                                                                                                                                                                                                      \n",
       "\n",
       "   overall  \n",
       "0  4.0      \n",
       "1  4.0      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "info(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove objective sentences for case B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "subj_filter = SubjectiveFilter()\n",
    "cases = {\n",
    "    'A': reviews,\n",
    "    'B': {\n",
    "        'train' : subj_filter.transform(reviews['train']),\n",
    "        'test': subj_filter.transform(reviews['test']),\n",
    "    }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create bag of words\n",
    "Remove accents  \n",
    "Tokenize  \n",
    "Lower the case  \n",
    "Apply custom stop words (keep all negations)  \n",
    "Remove all non alphabetic characters  \n",
    "Lematize  \n",
    " \n",
    "Output:  \n",
    "One list of words for each review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_dic['train']['positive'].iloc[:3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "word_bag = WordBag()\n",
    "\n",
    "for i in ['train','test']:\n",
    "    for j in ['positive','negative']:\n",
    "        train_test_dic[i][j]['words'] = \\\n",
    "            word_bag.create(train_test_dic[i][j]['reviewText'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove reviews that may not be on the movie, but on Amazon/support instead\n",
    "Input: \n",
    "* word tokens \n",
    "* one line per review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "about_movie = AboutMovie()\n",
    "movie_reviews = {'train':{}, 'test':{}}\n",
    "for i in ['train','test']:\n",
    "    for j in ['positive','negative']:\n",
    "         movie_reviews[i][j] = train_test_dic[i][j][[about_movie.check(words) \\\n",
    "                                                    for words in train_test_dic[i][j]['words']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_test_dic['test']['positive'][[not i for i in \\\n",
    "#                                     [about_movie.check(words) for words in train_test_dic[i][j]['words']]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_reviews = 0\n",
    "for i in ['train','test']:\n",
    "    for j in ['positive','negative']:\n",
    "        removed = train_test_dic[i][j].shape[0] - movie_reviews[i][j].shape[0]\n",
    "        tot_reviews += train_test_dic[i][j].shape[0]\n",
    "        print('Removed {0} ({1:.0%}) {2} {3} reviews'.format(removed, removed / train_test_dic[i][j].shape[0],\n",
    "                                                i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "import datetime\n",
    "if False:\n",
    "    currentDT = datetime.datetime.now()\n",
    "    for i in ['train','test']:\n",
    "        for j in ['positive','negative']:\n",
    "            train_test_dic[i][j].to_hdf(data_path + currentDT.strftime(\"%Y-%m-%d-%H-%M-%S\") + '_' + \n",
    "                      str(tot_reviews) + '_cleaned_reviews_before_B_' + i + '_' + j + '.pkl'\n",
    "              , key='df', mode='w', complevel=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    analyzer='word',       # Feed a list of words to TF-IDF\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None,\n",
    "    lowercase=False, \n",
    "    stop_words=None, \n",
    "    max_features=MAX_FEATURES,\n",
    "    norm='l2',            # normalize each review\n",
    "    use_idf=True)        # Keep high weight for most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPARSE = True\n",
    "\n",
    "if SPARSE:\n",
    "    # Optimization: add the review length while keeping sparse matrix\n",
    "    tf_train = tfidf.fit_transform(train_words)\n",
    "    tf_test = tfidf.transform(test_words)\n",
    "else:\n",
    "    tf_train = tfidf.fit_transform(train_words).todense()\n",
    "    tf_test = tfidf.transform(test_words).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(tfidf.vocabulary_))\n",
    "# tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add review length to modeling input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADD_LENGTH = False\n",
    "\n",
    "if ADD_LENGTH:\n",
    "    if SPARSE:\n",
    "        # Hack: pick an existing word to store the count\n",
    "        len_idx = 0\n",
    "        test_lengths = [len(words) for words in test_words]\n",
    "\n",
    "        for idx,words in enumerate(train_words):\n",
    "            tf_train[idx][len_idx] = len(words)\n",
    "        for idx,words in enumerate(test_words):\n",
    "            tf_test[idx][len_idx] = len(words)\n",
    "        X_train = tf_train\n",
    "        X_test = tf_test\n",
    "    else:\n",
    "        train_lengths = np.array([len(words) for words in train_words]).reshape(-1,1)\n",
    "        test_lengths = np.array([len(words) for words in test_words]).reshape(-1,1)\n",
    "        X_train = np.concatenate([tf_train, train_lengths],axis=1)\n",
    "        X_test = np.concatenate([tf_test, test_lengths],axis=1)\n",
    "else:\n",
    "    X_train = tf_train\n",
    "    X_test = tf_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if X_train.shape[0] != y_train.shape[0] or X_test.shape[0] != y_test.shape[0]:\n",
    "    print('@@@ Problem! @@@')\n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "    print(X_test.shape)\n",
    "    print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    pickle_out = open(data_path \n",
    "                      + 'tfidf_' \n",
    "                      + str(X_train.shape[0]) + 'Pos_Neg_Samples_'\n",
    "                      + str(X_train.shape[1]) + 'Feats.pkl'\n",
    "                      ,\"wb\")\n",
    "    pickle.dump(tfidf, pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Classifier for Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Classifier parameters\n",
    "# N_TREES = math.floor(np.sqrt(NB_SAMPLES) * 1.2)\n",
    "N_TREES = 500\n",
    "LEARN_RATE = 0.2\n",
    "MAX_DEPTH = 8\n",
    "MIN_IN_LEAF = 5 #7\n",
    "MAX_FEATURES = 'sqrt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier(learning_rate=LEARN_RATE, \n",
    "                                n_estimators=N_TREES, \n",
    "                                min_samples_leaf=MIN_IN_LEAF,\n",
    "                                max_depth=MAX_DEPTH,\n",
    "                                max_features=MAX_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    pickle.dump(gbc, open(data_path + 'GBC_'\n",
    "                       + str(NB_SAMPLES) + '_samples_'\n",
    "                       + str(N_TREES) + '_trees_' \n",
    "                       + str(LEARN_RATE) + '_lr_' \n",
    "                       + str(MAX_DEPTH) + '_maxdpth_'\n",
    "                       + str(MIN_IN_LEAF) + '_minleaf_'\n",
    "                       + str(MAX_FEATURES) + '_feats_'\n",
    "                       + '.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "print(MAX_FEATURES, ' features', N_TREES,'trees; ',\n",
    "      LEARN_RATE,'learn_rate; ', MAX_DEPTH, 'max_dpth; ',\n",
    "      MIN_IN_LEAF, 'min_in_leaf')\n",
    "classifier_report(gbc, X_train, y_train,\n",
    "                  'Gradient Boosting Classifier on training set')\n",
    "classifier_report(gbc, X_test, y_test, \n",
    "                  'Gradient Boosting Classifier on test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SAMPLE_FRACTION:', SAMPLE_FRACTION,'ADD_LENGTH:',ADD_LENGTH,' SPARSE:',SPARSE,' MAX_FEATURES:',MAX_FEATURES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    gb_pipe = Pipeline([('vect', tfidf), ('gb', gbc)])\n",
    "    gb_pipe.fit(X_train, y_train)\n",
    "    pickle.dump(gb_pipe, open('pickles/GBCpipe_balanced_comments_'\n",
    "                           + str(N_TREES) + '_trees_' \n",
    "                           + str(LEARN_RATE) + '_lr_' \n",
    "                           + str(MAX_DEPTH) + '_maxdpth_'\n",
    "                           + str(MIN_IN_LEAF) + '_minleaf_'\n",
    "                           + str(MAX_FEATURES) + '_feats_'\n",
    "                           + '.pkl', 'wb'))\n",
    "else:\n",
    "#     pickle_in = open(\"pickles/GBC_balanced_comments_300_trees_0.1_lr_15_maxdpth_2_minleaf_20000_feats_.pkl\",\n",
    "#                      \"rb\")\n",
    "#     gb_pipe = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    grid = {\n",
    "        'learning_rate': [.1,0.2,0.3],\n",
    "        'max_depth': [8],\n",
    "        'min_samples_leaf': [5],\n",
    "        'max_features': ['sqrt'],\n",
    "        'n_estimators': [300],\n",
    "        'random_state': [0]\n",
    "    }\n",
    "else:  # TEST\n",
    "    grid = {\n",
    "    'learning_rate': [1],\n",
    "    'max_depth': [2], \n",
    "    'min_samples_leaf': [2],\n",
    "#     'max_features': ['sqrt', None],\n",
    "    'n_estimators': [2],\n",
    "    'random_state': [0]\n",
    "}\n",
    "    \n",
    "# confusion_score = make_scorer(confusion_rmse, greater_is_better=False)\n",
    "\n",
    "gbc_grid_cv = GridSearchCV(\n",
    "    GradientBoostingClassifier(), \n",
    "    grid,\n",
    "    cv=4,  # number of folds\n",
    "    return_train_score=True,\n",
    "    verbose=1, \n",
    "    n_jobs=-1)\n",
    "gbc_grid_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gbc_grid_cv.predict(X_test)\n",
    "\n",
    "print('SAMPLE_FRACTION:', SAMPLE_FRACTION,'ADD_LENGTH:',ADD_LENGTH,' SPARSE:',SPARSE,' MAX_FEATURES:',MAX_FEATURES)\n",
    "\n",
    "print(gbc_grid_cv.best_params_)\n",
    "print(gbc_grid_cv.best_score_)\n",
    "res_df = pd.DataFrame(gbc_grid_cv.cv_results_)\n",
    "res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case B: with objective sentences removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split comments into separate sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reviews.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "test_reviews['sentence'] = test_reviews['reviewText'].map(sent_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reviews.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: update test_reviews in 2 places!\n",
    "sentences = test_reviews['sentence'] \\\n",
    ".apply(pd.Series) \\\n",
    ".merge(test_reviews, left_index = True, right_index = True) \\\n",
    ".drop(['sentence'], axis = 1) \\\n",
    ".melt(id_vars = ['reviewerID', 'asin','overall'], value_name = 'sentence') \\\n",
    ".drop(['variable'], axis = 1) \\\n",
    ".dropna()\n",
    "\n",
    "print(sentences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize along the word space of the obj-subj training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_model_path = '../obj_subj_dev/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(obj_model_path + 'fit_tfidf_vectorizer_for_obj_subj_sentences_classification.pkl', 'rb')\n",
    "obj_tfidf = pickle.load(pickle_in)\n",
    "pickle_in.close()\n",
    "len(obj_tfidf.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_X = obj_tfidf.transform(sentences['sentence']).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the obj-subj model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TREES = 100\n",
    "LEARN_RATE = 0.1\n",
    "MIN_IN_LEAF = 10\n",
    "pickle_in = open(obj_model_path + 'GBC_300_0.5_5.pkl', 'rb')\n",
    "obj_model = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = obj_model.predict(obj_X)\n",
    "print(len(y_test))\n",
    "y_test[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove objective sentences for case B using obj-subj model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences.shape)\n",
    "subjective_sentences = sentences[y_test == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = len(y_test) - len(subjective_sentences)\n",
    "\n",
    "display(Markdown('## => Removing {0} ({1:.0%}) objective sentences'\n",
    "                 .format(diff, diff/len(y_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality control: objective sentences discarded: really bad filter!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[y_test == 0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjective_sentences.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the sentences back into paragraph reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_groups = subjective_sentences.groupby(['reviewerID','asin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can't rely on words, won't be there\n",
    "def add_space(sentence):\n",
    "    return ' ' + sentence\n",
    "def merge_sentences(series):\n",
    "    s = series.map(add_space)\n",
    "    res = s.sum()\n",
    "#     print(res)\n",
    "#     print('##########')\n",
    "    return res\n",
    "\n",
    "subj_reviews_sentences = subj_groups['sentence'].agg(merge_sentences)\n",
    "print(type(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_reviews_stars = subj_groups['overall'].mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_reviews = pd.merge(subj_reviews_sentences, \n",
    "                        subj_reviews_stars, \n",
    "                        how='inner', on=['reviewerID', 'asin']).reset_index()\n",
    "subj_reviews.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_diff = len(test_reviews) - len(subj_review_comments)\n",
    "\n",
    "display(Markdown('## => Removing {0} ({1:.0%}) reviews with no emotional content'\n",
    "                 .format(review_diff, review_diff/len(test_reviews))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that stars still correspond to the right movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = pd.merge(subj_reviews, )\n",
    "test_reviews_groups = test_reviews.groupby(['reviewerID','asin'])\n",
    "\n",
    "test_reviews_stars = test_reviews_groups['overall'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = pd.merge(test_reviews_stars, subj_reviews,\n",
    "                 how='inner', on=['reviewerID', 'asin'])\n",
    "res = (check['overall_x'] == check['overall_y']).mean()\n",
    "if res == 1:\n",
    "    print('OK!')\n",
    "else:\n",
    "    print('### @@@@@@@@ PROBLEM! @@@@@@')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

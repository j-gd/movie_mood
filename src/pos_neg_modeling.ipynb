{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis to test:\n",
    "### Removing objective sentences from reviews helps predict star rating from reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import gzip\n",
    "import math\n",
    "import random\n",
    "from IPython.display import Markdown, display\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor, \\\n",
    "GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, \\\n",
    "classification_report, make_scorer\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# From this project\n",
    "from utils import rmse, rmse_train_cv, classifier_report, confusion_rmse\n",
    "from NLP import WordBag, AboutMovie\n",
    "\n",
    "\n",
    "# Avoid restarting Kernel\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "# %autosave 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsampling from Amazon reviews\n",
    "NB_SAMPLES = 360000 #4000  # up to 200k, then change the input file\n",
    "\n",
    "data_path = '../../datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.4*360"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get users' positive and negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name = '360000_balanced_train_test_reviews.pkl'\n",
    "file_name = '_balanced_pos_neg_train_test_reviews.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(data_path + str(NB_SAMPLES) + file_name,\"rb\")\n",
    "train_test_dic0 = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_FRACTION = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_dic = {'train': {}, 'test':{}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['train','test']:\n",
    "    for j in ['positive','negative']:\n",
    "         train_test_dic[i][j] = train_test_dic0[i][j] \\\n",
    "            .iloc[:math.floor(len(train_test_dic0[i][j].index) * SAMPLE_FRACTION), :] \\\n",
    "            .drop(['reviewerName', 'helpful', 'summary', 'unixReviewTime', 'reviewTime'], axis=1)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create bag of words\n",
    "Remove accents  \n",
    "Tokenize  \n",
    "Lower the case  \n",
    "Apply custom stop words (keep all negations)  \n",
    "Remove all non alphabetic characters  \n",
    "Lematize  \n",
    " \n",
    "Output:  \n",
    "One list of words for each review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1515619</th>\n",
       "      <td>A32244V7CQUBD6</td>\n",
       "      <td>B00005QFEK</td>\n",
       "      <td>This video actually focuses mostly on one of the characters that Emmanuelle (Krista Allen) is trying to teach about sex &amp; love. It's still pretty entertaining but if you are mostly interested in Kirsta Allen then you should know that she's not really in much of this episode.</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1515627</th>\n",
       "      <td>A32244V7CQUBD6</td>\n",
       "      <td>B00005QFEN</td>\n",
       "      <td>This episode pretty much has Hafron and Emmanuelle teleporting to different parts of the world and &amp;quot;doing it&amp;quot;. There is the continuing plot from an earlier episode of some group on Earth trying to track them down. That's the main reason for Emmanuelle and Hafron to jump to different parts of the world. Otherwise, this episode is mostly sex scenes.</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1515636</th>\n",
       "      <td>A33KKMGGVLZ29T</td>\n",
       "      <td>B00005QFER</td>\n",
       "      <td>This is an intimate concert of Robert Mirabal.  Although I thought that it was, as I said, masterful, the sound, at times sounded a little muffled.The storytelling of the songs gave an insight of native culture and of Mirabal's own family stories and history.The Dance and Ee You Oo are my picks for the best songs, but they are all a joy to watch.  The Rare Tribal Mob and the Mirabal Singers/Dancers are great and provide a mesmerising stage performance.Very enjoyable</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             reviewerID        asin  \\\n",
       "1515619  A32244V7CQUBD6  B00005QFEK   \n",
       "1515627  A32244V7CQUBD6  B00005QFEN   \n",
       "1515636  A33KKMGGVLZ29T  B00005QFER   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     reviewText  \\\n",
       "1515619  This video actually focuses mostly on one of the characters that Emmanuelle (Krista Allen) is trying to teach about sex & love. It's still pretty entertaining but if you are mostly interested in Kirsta Allen then you should know that she's not really in much of this episode.                                                                                                                                                                                                      \n",
       "1515627  This episode pretty much has Hafron and Emmanuelle teleporting to different parts of the world and &quot;doing it&quot;. There is the continuing plot from an earlier episode of some group on Earth trying to track them down. That's the main reason for Emmanuelle and Hafron to jump to different parts of the world. Otherwise, this episode is mostly sex scenes.                                                                                                                  \n",
       "1515636  This is an intimate concert of Robert Mirabal.  Although I thought that it was, as I said, masterful, the sound, at times sounded a little muffled.The storytelling of the songs gave an insight of native culture and of Mirabal's own family stories and history.The Dance and Ee You Oo are my picks for the best songs, but they are all a joy to watch.  The Rare Tribal Mob and the Mirabal Singers/Dancers are great and provide a mesmerising stage performance.Very enjoyable   \n",
       "\n",
       "         overall  \n",
       "1515619  4.0      \n",
       "1515627  4.0      \n",
       "1515636  4.0      "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_dic['train']['positive'].iloc[:3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "word_bag = WordBag()\n",
    "\n",
    "for i in ['train','test']:\n",
    "    for j in ['positive','negative']:\n",
    "        train_test_dic[i][j]['words'] = \\\n",
    "            word_bag.create(train_test_dic[i][j]['reviewText'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove reviews that may not be on the movie, but on Amazon/support instead\n",
    "Input: \n",
    "* word tokens \n",
    "* one line per review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "about_movie = AboutMovie()\n",
    "movie_reviews = {'train':{}, 'test':{}}\n",
    "for i in ['train','test']:\n",
    "    for j in ['positive','negative']:\n",
    "         movie_reviews[i][j] = train_test_dic[i][j][[about_movie.check(words) \\\n",
    "                                                    for words in train_test_dic[i][j]['words']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_test_dic['test']['positive'][[not i for i in \\\n",
    "#                                     [about_movie.check(words) for words in train_test_dic[i][j]['words']]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 17436 (30%) train positive reviews\n",
      "Removed 15647 (27%) train negative reviews\n",
      "Removed 4329 (30%) test positive reviews\n",
      "Removed 3969 (28%) test negative reviews\n"
     ]
    }
   ],
   "source": [
    "tot_reviews = 0\n",
    "for i in ['train','test']:\n",
    "    for j in ['positive','negative']:\n",
    "        removed = train_test_dic[i][j].shape[0] - movie_reviews[i][j].shape[0]\n",
    "        tot_reviews += train_test_dic[i][j].shape[0]\n",
    "        print('Removed {0} ({1:.0%}) {2} {3} reviews'.format(removed, removed / train_test_dic[i][j].shape[0],\n",
    "                                                i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "\n",
    "# if True:\n",
    "#     with bz2.open(data_path \n",
    "#                     + 'movie_reviews_' \n",
    "#                     + str(tot_reviews) + 'Pos_Neg_Samples.pkl.bz2'\n",
    "#                     , \"wt\") as f:\n",
    "#          # Write compressed data to file\n",
    "#         unused = f.write(str(movie_reviews))\n",
    "    \n",
    "# if True:\n",
    "# #     tot_reviews = 4000\n",
    "#     with bz2.open(data_path \n",
    "#                     + 'movie_reviews_' \n",
    "#                     + str(tot_reviews) + 'Pos_Neg_Samples.pkl.bz2'\n",
    "#                     , \"rt\") as f:\n",
    "#          # Write compressed data to file\n",
    "#         test = f.read()\n",
    "        \n",
    "if False:\n",
    "    pickle_out = open(data_path\n",
    "                    + 'movie_reviews_' \n",
    "                    + str(tot_reviews) + 'Pos_Neg_Samples.pkl'\n",
    "                    , \"wb\")\n",
    "    pickle.dump(movie_reviews, pickle_out)\n",
    "    pickle_out.close()\n",
    " \n",
    "if False:\n",
    "    pickle_in = open(data_path\n",
    "                    + 'movie_reviews_' \n",
    "                    + str(tot_reviews) + 'Pos_Neg_Samples.pkl'\n",
    "                    , \"rb\")\n",
    "    test_dic = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do: investigate improvements to negative sentences \n",
    "For example:  \n",
    "merge negations with next word, remove next word  \n",
    "Or lemmatize based on grammar:  https://www.machinelearningplus.com/nlp/lemmatization-examples-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    analyzer='word',       # Feed a list of words to TF-IDF\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None,\n",
    "    lowercase=False, \n",
    "    stop_words=None, \n",
    "    max_features=MAX_FEATURES,\n",
    "    norm='l2',            # normalize each review\n",
    "    use_idf=True)        # Keep high weight for most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words = pd.concat([movie_reviews['train']['positive']['words'],\n",
    "                     movie_reviews['train']['negative']['words']])\n",
    "y_train = np.concatenate([np.ones((movie_reviews['train']['positive'].shape[0],)), \n",
    "                          np.zeros((movie_reviews['train']['negative'].shape[0],))])\n",
    "test_words = pd.concat([movie_reviews['test']['positive']['words'],\n",
    "                     movie_reviews['test']['negative']['words']])\n",
    "y_test = np.concatenate([np.ones((movie_reviews['test']['positive'].shape[0],)), \n",
    "                          np.zeros((movie_reviews['test']['negative'].shape[0],))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPARSE = True\n",
    "\n",
    "if SPARSE:\n",
    "    # Optimization: add the review length while keeping sparse matrix\n",
    "    tf_train = tfidf.fit_transform(train_words)\n",
    "    tf_test = tfidf.transform(test_words)\n",
    "else:\n",
    "    tf_train = tfidf.fit_transform(train_words).todense()\n",
    "    tf_test = tfidf.transform(test_words).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(tfidf.vocabulary_))\n",
    "# tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add review length to modeling input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADD_LENGTH = False\n",
    "\n",
    "if ADD_LENGTH:\n",
    "    if SPARSE:\n",
    "        # Hack: pick an existing word to store the count\n",
    "        len_idx = 0\n",
    "        test_lengths = [len(words) for words in test_words]\n",
    "\n",
    "        for idx,words in enumerate(train_words):\n",
    "            tf_train[idx][len_idx] = len(words)\n",
    "        for idx,words in enumerate(test_words):\n",
    "            tf_test[idx][len_idx] = len(words)\n",
    "        X_train = tf_train\n",
    "        X_test = tf_test\n",
    "    else:\n",
    "        train_lengths = np.array([len(words) for words in train_words]).reshape(-1,1)\n",
    "        test_lengths = np.array([len(words) for words in test_words]).reshape(-1,1)\n",
    "        X_train = np.concatenate([tf_train, train_lengths],axis=1)\n",
    "        X_test = np.concatenate([tf_test, test_lengths],axis=1)\n",
    "else:\n",
    "    X_train = tf_train\n",
    "    X_test = tf_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "if X_train.shape[0] != y_train.shape[0] or X_test.shape[0] != y_test.shape[0]:\n",
    "    print('@@@ Problem! @@@')\n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "    print(X_test.shape)\n",
    "    print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    pickle_out = open(data_path \n",
    "                      + 'tfidf_' \n",
    "                      + str(X_train.shape[0]) + 'Pos_Neg_Samples_'\n",
    "                      + str(X_train.shape[1]) + 'Feats.pkl'\n",
    "                      ,\"wb\")\n",
    "    pickle.dump(tfidf, pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Classifier for Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Classifier parameters\n",
    "# N_TREES = math.floor(np.sqrt(NB_SAMPLES) * 1.2)\n",
    "N_TREES = 500\n",
    "LEARN_RATE = 0.2\n",
    "MAX_DEPTH = 8\n",
    "MIN_IN_LEAF = 5 #7\n",
    "MAX_FEATURES = 'sqrt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier(learning_rate=LEARN_RATE, \n",
    "                                n_estimators=N_TREES, \n",
    "                                min_samples_leaf=MIN_IN_LEAF,\n",
    "                                max_depth=MAX_DEPTH,\n",
    "                                max_features=MAX_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.2, loss='deviance', max_depth=8,\n",
       "              max_features='sqrt', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=5, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
       "              n_iter_no_change=None, presort='auto', random_state=None,\n",
       "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    pickle.dump(gbc, open(data_path + 'GBC_'\n",
    "                       + str(NB_SAMPLES) + '_samples_'\n",
    "                       + str(N_TREES) + '_trees_' \n",
    "                       + str(LEARN_RATE) + '_lr_' \n",
    "                       + str(MAX_DEPTH) + '_maxdpth_'\n",
    "                       + str(MIN_IN_LEAF) + '_minleaf_'\n",
    "                       + str(MAX_FEATURES) + '_feats_'\n",
    "                       + '.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sqrt  features 500 trees;  0.2 learn_rate;  8 max_dpth;  5 min_in_leaf\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Report for Gradient Boosting Classifier on training set:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Off diagonal: 5%"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Confusion Matrix:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[39599  2354]\n",
      " [ 1777 38387]]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Classification Report:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.94      0.95     41953\n",
      "         1.0       0.94      0.96      0.95     40164\n",
      "\n",
      "   micro avg       0.95      0.95      0.95     82117\n",
      "   macro avg       0.95      0.95      0.95     82117\n",
      "weighted avg       0.95      0.95      0.95     82117\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Report for Gradient Boosting Classifier on test set:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Off diagonal: 11%"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Confusion Matrix:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9149 1282]\n",
      " [1001 9070]]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Classification Report:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.88      0.89     10431\n",
      "         1.0       0.88      0.90      0.89     10071\n",
      "\n",
      "   micro avg       0.89      0.89      0.89     20502\n",
      "   macro avg       0.89      0.89      0.89     20502\n",
      "weighted avg       0.89      0.89      0.89     20502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "print(MAX_FEATURES, ' features', N_TREES,'trees; ',\n",
    "      LEARN_RATE,'learn_rate; ', MAX_DEPTH, 'max_dpth; ',\n",
    "      MIN_IN_LEAF, 'min_in_leaf')\n",
    "classifier_report(gbc, X_train, y_train,\n",
    "                  'Gradient Boosting Classifier on training set')\n",
    "classifier_report(gbc, X_test, y_test, \n",
    "                  'Gradient Boosting Classifier on test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE_FRACTION: 0.4 ADD_LENGTH: False  SPARSE: True  MAX_FEATURES: sqrt\n"
     ]
    }
   ],
   "source": [
    "print('SAMPLE_FRACTION:', SAMPLE_FRACTION,'ADD_LENGTH:',ADD_LENGTH,' SPARSE:',SPARSE,' MAX_FEATURES:',MAX_FEATURES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model tuning\n",
    "Find the maximum size the machine will take (samples * features)\n",
    "Accuracy increases by 2% when increasing dataset from 36K to 72K samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate what went wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = gbc.predict(X_test)\n",
    "true_0_pred_1 = (test_pred == 1) & (y_test == 0)\n",
    "print(np.unique(true_0_pred_1, return_counts=True))\n",
    "print(len(true_0_pred_1))\n",
    "print(len(X_test))\n",
    "X_test[true_0_pred_1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reviews = pd.concat([movie_reviews['test']['positive'],\n",
    "                     movie_reviews['test']['negative']])\n",
    "test_reviews[true_0_pred_1]['reviewText'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    gb_pipe = Pipeline([('vect', tfidf), ('gb', gbc)])\n",
    "    gb_pipe.fit(X_train, y_train)\n",
    "    pickle.dump(gb_pipe, open('pickles/GBCpipe_balanced_comments_'\n",
    "                           + str(N_TREES) + '_trees_' \n",
    "                           + str(LEARN_RATE) + '_lr_' \n",
    "                           + str(MAX_DEPTH) + '_maxdpth_'\n",
    "                           + str(MIN_IN_LEAF) + '_minleaf_'\n",
    "                           + str(MAX_FEATURES) + '_feats_'\n",
    "                           + '.pkl', 'wb'))\n",
    "else:\n",
    "#     pickle_in = open(\"pickles/GBC_balanced_comments_300_trees_0.1_lr_15_maxdpth_2_minleaf_20000_feats_.pkl\",\n",
    "#                      \"rb\")\n",
    "#     gb_pipe = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 3 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:  3.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise-deprecating',\n",
       "       estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_sampl...      subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'learning_rate': [0.1, 0.2, 0.3], 'max_depth': [8], 'min_samples_leaf': [5], 'max_features': ['sqrt'], 'n_estimators': [300], 'random_state': [0]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if True:\n",
    "    grid = {\n",
    "        'learning_rate': [.1,0.2,0.3],\n",
    "        'max_depth': [8],\n",
    "        'min_samples_leaf': [5],\n",
    "        'max_features': ['sqrt'],\n",
    "        'n_estimators': [300],\n",
    "        'random_state': [0]\n",
    "    }\n",
    "else:  # TEST\n",
    "    grid = {\n",
    "    'learning_rate': [1],\n",
    "    'max_depth': [2], \n",
    "    'min_samples_leaf': [2],\n",
    "#     'max_features': ['sqrt', None],\n",
    "    'n_estimators': [2],\n",
    "    'random_state': [0]\n",
    "}\n",
    "    \n",
    "# confusion_score = make_scorer(confusion_rmse, greater_is_better=False)\n",
    "\n",
    "gbc_grid_cv = GridSearchCV(\n",
    "    GradientBoostingClassifier(), \n",
    "    grid,\n",
    "    cv=4,  # number of folds\n",
    "    return_train_score=True,\n",
    "    verbose=1, \n",
    "    n_jobs=-1)\n",
    "gbc_grid_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gbc_grid_cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE_FRACTION: 0.4 ADD_LENGTH: False  SPARSE: True  MAX_FEATURES: sqrt\n"
     ]
    }
   ],
   "source": [
    "print('SAMPLE_FRACTION:', SAMPLE_FRACTION,'ADD_LENGTH:',ADD_LENGTH,' SPARSE:',SPARSE,' MAX_FEATURES:',MAX_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.2, 'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 300, 'random_state': 0}\n",
      "0.8749832555987189\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>param_min_samples_leaf</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>param_random_state</th>\n",
       "      <th>...</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>77.948625</td>\n",
       "      <td>0.824304</td>\n",
       "      <td>0.829178</td>\n",
       "      <td>0.016242</td>\n",
       "      <td>0.1</td>\n",
       "      <td>8</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>5</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.857908</td>\n",
       "      <td>0.870185</td>\n",
       "      <td>0.008076</td>\n",
       "      <td>3</td>\n",
       "      <td>0.917255</td>\n",
       "      <td>0.915812</td>\n",
       "      <td>0.914480</td>\n",
       "      <td>0.917143</td>\n",
       "      <td>0.916172</td>\n",
       "      <td>0.001130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72.274271</td>\n",
       "      <td>0.822555</td>\n",
       "      <td>0.746028</td>\n",
       "      <td>0.013268</td>\n",
       "      <td>0.2</td>\n",
       "      <td>8</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>5</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.864143</td>\n",
       "      <td>0.874983</td>\n",
       "      <td>0.006984</td>\n",
       "      <td>1</td>\n",
       "      <td>0.939776</td>\n",
       "      <td>0.940362</td>\n",
       "      <td>0.938478</td>\n",
       "      <td>0.941141</td>\n",
       "      <td>0.939939</td>\n",
       "      <td>0.000973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70.229492</td>\n",
       "      <td>0.776846</td>\n",
       "      <td>0.641123</td>\n",
       "      <td>0.112207</td>\n",
       "      <td>0.3</td>\n",
       "      <td>8</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>5</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866189</td>\n",
       "      <td>0.874861</td>\n",
       "      <td>0.005485</td>\n",
       "      <td>2</td>\n",
       "      <td>0.952490</td>\n",
       "      <td>0.950299</td>\n",
       "      <td>0.951695</td>\n",
       "      <td>0.952442</td>\n",
       "      <td>0.951731</td>\n",
       "      <td>0.000885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0  77.948625      0.824304      0.829178         0.016242         \n",
       "1  72.274271      0.822555      0.746028         0.013268         \n",
       "2  70.229492      0.776846      0.641123         0.112207         \n",
       "\n",
       "  param_learning_rate param_max_depth param_max_features  \\\n",
       "0  0.1                 8               sqrt                \n",
       "1  0.2                 8               sqrt                \n",
       "2  0.3                 8               sqrt                \n",
       "\n",
       "  param_min_samples_leaf param_n_estimators param_random_state  ...  \\\n",
       "0  5                      300                0                  ...   \n",
       "1  5                      300                0                  ...   \n",
       "2  5                      300                0                  ...   \n",
       "\n",
       "  split3_test_score  mean_test_score  std_test_score  rank_test_score  \\\n",
       "0  0.857908          0.870185         0.008076        3                 \n",
       "1  0.864143          0.874983         0.006984        1                 \n",
       "2  0.866189          0.874861         0.005485        2                 \n",
       "\n",
       "   split0_train_score  split1_train_score  split2_train_score  \\\n",
       "0  0.917255            0.915812            0.914480             \n",
       "1  0.939776            0.940362            0.938478             \n",
       "2  0.952490            0.950299            0.951695             \n",
       "\n",
       "   split3_train_score  mean_train_score  std_train_score  \n",
       "0  0.917143            0.916172          0.001130         \n",
       "1  0.941141            0.939939          0.000973         \n",
       "2  0.952442            0.951731          0.000885         \n",
       "\n",
       "[3 rows x 24 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(gbc_grid_cv.best_params_)\n",
    "print(gbc_grid_cv.best_score_)\n",
    "res_df = pd.DataFrame(gbc_grid_cv.cv_results_)\n",
    "res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case B: with objective sentences removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split comments into separate sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "small['sentence'] = small['reviewText'].map(sent_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small.drop(['reviewerName', 'helpful', 'reviewText', 'summary', \n",
    "            'unixReviewTime', 'reviewTime'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = small['sentence'] \\\n",
    ".apply(pd.Series) \\\n",
    ".merge(small, left_index = True, right_index = True) \\\n",
    ".drop(['sentence'], axis = 1) \\\n",
    ".melt(id_vars = ['reviewerID', 'asin','overall'], value_name = 'sentence') \\\n",
    ".drop(['variable'], axis = 1) \\\n",
    ".dropna()\n",
    "\n",
    "print(sentences.shape)\n",
    "sentences.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence-level prep & cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "from utils import split_n_lower, not_about_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into words and lower the case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences['words'] = sentences['sentence'].apply(lambda s: split_n_lower(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences.shape)\n",
    "sentences.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep support-related sentences as they probably have impact on rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on_movies_filter = [not_about_support(word) for word in sentences['words']]\n",
    "sentences_on_movie = sentences #[on_movies_filter]\n",
    "\n",
    "print('Removing {} records'.format(sentences.shape[0]- sentences_on_movie.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_on_movie.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base case: A reviews with objective and subjective sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel dies here at 50K samples\n",
    "all_reviews_groups = sentences_on_movie.groupby(['reviewerID','asin'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews_stars = all_reviews_groups['overall'].mean()\n",
    "all_reviews_stars[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews_comments = all_reviews_groups['words'].sum()\n",
    "print(sentences_on_movie.iloc[0, 4])\n",
    "print(all_reviews_comments.shape)\n",
    "print(all_reviews_comments[0])\n",
    "len(all_reviews_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove objective sentences for case B using obj-subj model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_colwidth', -1)\n",
    "# sentences_on_movie['sentence']\n",
    "sentences_on_movie.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_on_movie.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize along the word space of the obj-subj training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = pickle.load(open('pickles/Obj-Subj_tfidf.pkl', 'rb'))\n",
    "len(tfidf.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_tfidf = tfidf.transform(sentences_on_movie['sentence']).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply the obj-subj model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TREES = 100\n",
    "LEARN_RATE = 0.1\n",
    "MIN_IN_LEAF = 10\n",
    "pickle_in = open('pickles/GBC_'+ str(N_TREES) +'_' + str(LEARN_RATE) \n",
    "                        +'_' + str(MIN_IN_LEAF) + '_20min.pkl', 'rb')\n",
    "gb_model = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = gb_model.predict(sentences_tfidf)\n",
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjective_sentences = sentences_on_movie[y_test == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown('### Removing {} objective sentences'\n",
    "                 .format(len(y_test) - len(subjective_sentences))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjective_sentences.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge the sentences back into paragraph reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_groups = subjective_sentences.groupby(['reviewerID','asin'])\n",
    "subj_reviews_stars = subj_groups['overall'].mean()\n",
    "# subjective_reviewssubjective_reviews['sentence'].apply(lambda x: x.sum())\n",
    "# subjective_reviews_reviews = \n",
    "subj_reviews_stars[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_review_comments = subj_groups['words'].sum()\n",
    "print(subj_review_comments.shape)\n",
    "print(subj_review_comments[0])\n",
    "subj_review_comments[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that stars still correspond to the right movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 6000\n",
    "end = 6010\n",
    "all_reviews_comments.loc[('A33Z7JTV7SSW9Y', '0718000315')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_reviews_stars.loc[('A33Z7JTV7SSW9Y', '0718000315')])\n",
    "print(sentences_on_movie.loc[sentences_on_movie['reviewerID']=='A33Z7JTV7SSW9Y']) \n",
    "# and sentences_on_movie['asin']=='0718000315'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = -1\n",
    "print(small.loc[small['reviewerID']=='A33Z7JTV7SSW9Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_on_movie[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create emotion vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of reviews:', all_reviews_comments.shape[0])\n",
    "print('Total number of subjective reviews:', subj_review_comments.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "from emotions_seven import Emotions7\n",
    "emote = Emotions7()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews_emotions = emote.vectorize(all_reviews_comments)\n",
    "print(all_reviews_emotions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emote.emotions_in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews_emotions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_revs_with_emotions = all_reviews_emotions[emote.emotions_in_text == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(all_revs_with_emotions.shape)\n",
    "# all_revs_stars = all_reviews_stars[emote.emotions_in_text]\n",
    "all_reviews_emotions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_reviews_emotions = emote.vectorize(subj_review_comments)\n",
    "print(subj_reviews_emotions.shape)\n",
    "subj_reviews_emotions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a model on base case (all comments) for star rating prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_subj_train, X_subj_cv, y_subj_train, y_subj_cv = train_test_split(\n",
    "    subj_reviews_emotions, subj_reviews_stars, test_size=0.2, random_state=0)\n",
    "X_subj_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_subj = GradientBoostingClassifier(learning_rate=LEARN_RATE, \n",
    "                                n_estimators=N_TREES, \n",
    "                                min_samples_leaf=MIN_IN_LEAF,\n",
    "                                random_state=0)\n",
    "gbc_subj.fit(X_subj_train, y_subj_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Gradient Boosting Classifier')\n",
    "print('Training score using all comments: {0:.2f}'\n",
    "      .format(gbc_all.score(X_train, y_train)))\n",
    "print('CV score using all comments: {0:.2f}'\n",
    "      .format(gbc_all.score(X_cv, y_cv)))\n",
    "print('')\n",
    "\n",
    "# print('Training score using subjective comments only: {0:.2f}'\n",
    "#       .format(gbc_subj.score(X_subj_train, y_subj_train)))\n",
    "# print('CV score using subjective comments only: {0:.2f}'\n",
    "#       .format(gbc_subj.score(X_subj_cv, y_subj_cv)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# lr = LogisticRegression(random_state=0, solver='lbfgs',\n",
    "#                        multi_class='multinomial',max_iter=1000)\n",
    "# lr.fit(X_subj_train, y_subj_train)\n",
    "# print(lr.score(X_subj_train, y_subj_train))\n",
    "# print(lr.score(X_subj_cv, y_subj_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_all = sm.OLS(y_train, X_train)\n",
    "results_all = ols_all.fit()\n",
    "results_all.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ols_subj = sm.OLS(y_subj_train, X_subj_train)\n",
    "results_subj = ols_subj.fit()\n",
    "results_subj.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "all_reviews_emotions, all_reviews_stars\n",
    "\n",
    "sns.heatmap(raw_df.corr(), annot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

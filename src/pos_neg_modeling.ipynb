{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis to test: \n",
    "### Removing objective sentences from reviews helps predict star rating from reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import gzip\n",
    "import math\n",
    "import random\n",
    "from IPython.display import Markdown, display\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor, \\\n",
    "GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, \\\n",
    "classification_report, make_scorer\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# From this project\n",
    "from utils import rmse, rmse_train_cv, classifier_report, confusion_rmse\n",
    "from NLPv0 import WordBag, AboutMovie\n",
    "\n",
    "\n",
    "# Avoid restarting Kernel\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "# %autosave 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "word_bag = WordBag()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsampling from Amazon reviews\n",
    "NB_SAMPLES = 360000 #4000  # up to 200k, then change the input file\n",
    "\n",
    "data_path = '../../datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.4*360"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get users' positive and negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name = '360000_balanced_train_test_reviews.pkl'\n",
    "file_name = '_balanced_pos_neg_train_test_reviews.pkl'\n",
    "\n",
    "pickle_in = open(data_path + str(NB_SAMPLES) + file_name,\"rb\")\n",
    "train_test_dic0 = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_FRACTION = 0.4\n",
    "\n",
    "train_test_dic = {'train': {}, 'test':{}}\n",
    "\n",
    "for i in ['train','test']:\n",
    "    for j in ['positive','negative']:\n",
    "         train_test_dic[i][j] = train_test_dic0[i][j] \\\n",
    "            .iloc[:math.floor(len(train_test_dic0[i][j].index) * SAMPLE_FRACTION), :] \\\n",
    "            .drop(['reviewerName', 'helpful', 'summary', 'unixReviewTime', 'reviewTime'], axis=1)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create bag of words\n",
    "Remove accents  \n",
    "Tokenize  \n",
    "Lower the case  \n",
    "Apply custom stop words (keep all negations)  \n",
    "Remove all non alphabetic characters  \n",
    "Lematize  \n",
    " \n",
    "Output:  \n",
    "One list of words for each review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_dic['train']['positive'].iloc[:3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in ['train','test']:\n",
    "#     for j in ['positive','negative']:\n",
    "#         train_test_dic[i][j]['words'] = \\\n",
    "#             word_bag.create(train_test_dic[i][j]['reviewText'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove reviews that may not be on the movie, but on Amazon/support instead\n",
    "Input: \n",
    "* word tokens \n",
    "* one line per review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reload_ext autoreload\n",
    "# about_movie = AboutMovie()\n",
    "# movie_reviews = {'train':{}, 'test':{}}\n",
    "# for i in ['train','test']:\n",
    "#     for j in ['positive','negative']:\n",
    "#          movie_reviews[i][j] = train_test_dic[i][j][[about_movie.check(words) \\\n",
    "#                                                     for words in train_test_dic[i][j]['words']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_test_dic['test']['positive'][[not i for i in \\\n",
    "#                                     [about_movie.check(words) for words in train_test_dic[i][j]['words']]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tot_reviews = 0\n",
    "# for i in ['train','test']:\n",
    "#     for j in ['positive','negative']:\n",
    "#         removed = train_test_dic[i][j].shape[0] - movie_reviews[i][j].shape[0]\n",
    "#         tot_reviews += train_test_dic[i][j].shape[0]\n",
    "#         print('Removed {0} ({1:.0%}) {2} {3} reviews'.format(removed, removed / train_test_dic[i][j].shape[0],\n",
    "#                                                 i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import bz2\n",
    "\n",
    "# if True:\n",
    "#     with bz2.open(data_path \n",
    "#                     + 'movie_reviews_' \n",
    "#                     + str(tot_reviews) + 'Pos_Neg_Samples.pkl.bz2'\n",
    "#                     , \"wt\") as f:\n",
    "#          # Write compressed data to file\n",
    "#         unused = f.write(str(movie_reviews))\n",
    "    \n",
    "# if True:\n",
    "# #     tot_reviews = 4000\n",
    "#     with bz2.open(data_path \n",
    "#                     + 'movie_reviews_' \n",
    "#                     + str(tot_reviews) + 'Pos_Neg_Samples.pkl.bz2'\n",
    "#                     , \"rt\") as f:\n",
    "#          # Write compressed data to file\n",
    "#         test = f.read()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1515619</th>\n",
       "      <td>A32244V7CQUBD6</td>\n",
       "      <td>B00005QFEK</td>\n",
       "      <td>This video actually focuses mostly on one of the characters that Emmanuelle (Krista Allen) is trying to teach about sex &amp; love. It's still pretty entertaining but if you are mostly interested in Kirsta Allen then you should know that she's not really in much of this episode.</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             reviewerID        asin  \\\n",
       "1515619  A32244V7CQUBD6  B00005QFEK   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                  reviewText  \\\n",
       "1515619  This video actually focuses mostly on one of the characters that Emmanuelle (Krista Allen) is trying to teach about sex & love. It's still pretty entertaining but if you are mostly interested in Kirsta Allen then you should know that she's not really in much of this episode.   \n",
       "\n",
       "         overall  \n",
       "1515619  4.0      "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = train_test_dic['train']['positive']\n",
    "df1[(df1['reviewerID'] == 'A32244V7CQUBD6') & (df1['asin'] == 'B00005QFEK')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'movie_reviews' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a9ac90972d71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmovie_reviews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'positive'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reviewerID'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'A32244V7CQUBD6'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'asin'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'B00005QFEK'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'movie_reviews' is not defined"
     ]
    }
   ],
   "source": [
    "# df1 = movie_reviews['train']['positive']\n",
    "# df1[(df1['reviewerID'] == 'A32244V7CQUBD6') & (df1['asin'] == 'B00005QFEK')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movie_reviews = train_test_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For case B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    pickle_out = open(data_path\n",
    "                    + 'movie_reviews_' \n",
    "                    + str(tot_reviews) + 'Pos_Neg_Samples.pkl'\n",
    "                    , \"wb\")\n",
    "    pickle.dump(movie_reviews, pickle_out)\n",
    "    pickle_out.close()\n",
    " \n",
    "if True:\n",
    "    pickle_in = open(data_path\n",
    "                    + 'reviews_wout_most_subj_0.15.pkl'\n",
    "                    , \"rb\")\n",
    "    movie_reviews = pickle.load(pickle_in)\n",
    "    pickle_in.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    for i in ['train','test']:\n",
    "        for j in ['positive','negative']:\n",
    "            movie_reviews[i][j]['words'] = \\\n",
    "                word_bag.create(movie_reviews[i][j]['reviewText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57600, 4)\n",
      "(57600, 4)\n",
      "(14400, 4)\n",
      "(14400, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "144000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = 0\n",
    "for i in ['train','test']:\n",
    "    for j in ['positive','negative']:\n",
    "        print(train_test_dic[i][j].shape)\n",
    "        total += train_test_dic[i][j].shape[0]\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76097, 4)\n",
      "(74227, 4)\n",
      "(19075, 4)\n",
      "(17354, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "186753"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = 0\n",
    "for i in ['train','test']:\n",
    "    for j in ['positive','negative']:\n",
    "        print(movie_reviews[i][j].shape)\n",
    "        total += movie_reviews[i][j].shape[0]\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1515619</th>\n",
       "      <td>A32244V7CQUBD6</td>\n",
       "      <td>B00005QFEK</td>\n",
       "      <td>This video actually focuses mostly on one of the characters that Emmanuelle (Krista Allen) is trying to teach about sex &amp; love. It's still pretty entertaining but if you are mostly interested in Kirsta Allen then you should know that she's not really in much of this episode.</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1515627</th>\n",
       "      <td>A32244V7CQUBD6</td>\n",
       "      <td>B00005QFEN</td>\n",
       "      <td>This episode pretty much has Hafron and Emmanuelle teleporting to different parts of the world and &amp;quot;doing it&amp;quot;. There is the continuing plot from an earlier episode of some group on Earth trying to track them down. That's the main reason for Emmanuelle and Hafron to jump to different parts of the world. Otherwise, this episode is mostly sex scenes.</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1515636</th>\n",
       "      <td>A33KKMGGVLZ29T</td>\n",
       "      <td>B00005QFER</td>\n",
       "      <td>This is an intimate concert of Robert Mirabal.  Although I thought that it was, as I said, masterful, the sound, at times sounded a little muffled.The storytelling of the songs gave an insight of native culture and of Mirabal's own family stories and history.The Dance and Ee You Oo are my picks for the best songs, but they are all a joy to watch.  The Rare Tribal Mob and the Mirabal Singers/Dancers are great and provide a mesmerising stage performance.Very enjoyable</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1515678</th>\n",
       "      <td>A33P47VEH0YULL</td>\n",
       "      <td>B00005QG2N</td>\n",
       "      <td>A well put together DVD for the \"Stinkiest band\" in the world.  Easy to navigate, and with some pretty interesting side notes...a great collectors item for any fan of Cradle of Filth or Swedish metal.</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1515690</th>\n",
       "      <td>ADG33WELAQRZJ</td>\n",
       "      <td>B00005QG2N</td>\n",
       "      <td>Great show from the inventors of the extreme gothic metal genre !! 75'of pure mayhem with good visuals and a great sound, although the vocals are a little high in the mix for my taste and the guitar on the right channel is 10 times louder than the one on the left so I ended up switching it to mono to even things up. It's hard to give this more stars esp. when you've watched the PTSFirepower DVD first like I did, that one is so much better. The 5 bonus videoclips totally rule and round up the package making the purchase a worthy one.Now the mockumentary may not be suited to everyone's tastes, personally it bored me and know of a lot of people who feels it ruins the band's image. Well don't watch it then.</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1515691</th>\n",
       "      <td>A3YXITJWFW4BW</td>\n",
       "      <td>B00005QG2N</td>\n",
       "      <td>I rented this dvd, i didnt buy it.  I also rented PanDamonAeon and Mannequin.  This dvd has excellent very long concert footage.  Its practically the same songs as on Livebait for the Dead.  And then we have the rest of the dvd.  Im sorry, i love CoF but that \"schockumentary\" thing was just sad.  I could barely stand to watch it.  It's stupid and boring, maybe if theyd shown more of the band in a more coherent manner...and it just goes on and on forever.  This is a 2 hour plus dvd adn half of it is crap.  The \"blair twit Project\" is slightly amusing, but mostly boring.  The music videos are good though.  I would still recommend buying this dvd just for the music videos and the concert footage.  PanDamonAeon gave me more joy though.</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1515700</th>\n",
       "      <td>A214NHULS3H7OX</td>\n",
       "      <td>B00005QG2N</td>\n",
       "      <td>Excellent Sound, Excellent Picture, Tons and Tons of songs. Love the Scorched Earth Erotic video. The only reason I didn't give this 5 stars is because I don't think they let their female singer sing enough, it adds a nice goth touch. I also did not give it five stars because it would have been nice to have the \"her ghost in the fog\" video on here, and finally I thin Dani could have done better on some parts of the songs. For example: On \"her ghost in the fog\" (live version), he chooses to growl in that typical death metal voice, instead of talking in the deep gothic voice like he does on the album. By using this growly voice, I think he is cheese'n off some of the songs best parts. Anyway, these are minor gripes and this is a must have DVD, so definitely BUY IT!...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1515701</th>\n",
       "      <td>A2IZOU2G1QX0JD</td>\n",
       "      <td>B00005QG2N</td>\n",
       "      <td>As most of my friends hate, I'm massively into Cradle of Filth.  In the style of Napoleon Dynamite describing a Liger, I would say \"...they're pretty much my favorite band.\"  Their music is among the worst in the industry, they suck in a way few bands can meaningfully aspire to.  I've seen them live, bought pretty much every t-shirt from every album and tour, and have almost every album that they've released over the last \"decade and a bit\".  I love it, I love it all.  I recently purchased Heavy, Left-Handed, &amp; Candid.  If you don't like Cradle of Filth, then you'll most certainly hate this DVD.  It's absolutely fantastic, full of everything I was hoping for.There is a bunch of documentary type stuff, eleven live performances, a couple of music videos, and a few other \"shorts\" put together by the band.  Another one of the bonus items they threw on the disc is a short movie called \"The Blair Twit Project\".  This is basically footage of the band walking through woodland, filmed entirely using the night-vision mode on a video camera as they follow the trail of \"a pissed northern bastard.\"  Being a \"northerner\" myself, it felt very much like a standard night walking home from the pub on a Friday night being idiots.  It's funny to watch *once*.The DVD is actually really well put together and there is a ton of material included, which is a refreshing change when compared to some other music DVDs I've bought that barely even take the time to put a decent menu together.  As with their music, most people will hate this DVD.  I loved it, but then again, I'm weird.  The live performances are pretty awesome and it has solidified my decision to have to see this band again soon.  I don't know if they come to the US very often if at all, but some guy in Hot Topic assured me that he'd seen them here a few years ago.  I'll have to keep my eyes on the tour dates.  Then the hardest job will be convincing anyone to actually go with me.  My wife say's she's up for it, but I think that will have to be seen to be believed.</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1515707</th>\n",
       "      <td>ARUHLQVP23GQ1</td>\n",
       "      <td>B00005QIVC</td>\n",
       "      <td>This category 3 classic mixes genres like only Hong Kong use to be able to do before the takeover. It over priced because of it's rarity so unless you collect rare Category 3 films you might want to get something else. As Category 3 films go this is entertaining but not as over the top as Dr. lamb, Red to Kill or the Untold Story.</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1515710</th>\n",
       "      <td>ARYXA0US2VB5Q</td>\n",
       "      <td>B00005QIVE</td>\n",
       "      <td>I remember this when i had it on video cd,it was a 2 cd set.I got because i wanted to know more about the asian culture.The story was strange,somewhat brutal &amp; interesting.There is nudity for the guys here.I learned much from the movies.</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             reviewerID        asin  \\\n",
       "1515619  A32244V7CQUBD6  B00005QFEK   \n",
       "1515627  A32244V7CQUBD6  B00005QFEN   \n",
       "1515636  A33KKMGGVLZ29T  B00005QFER   \n",
       "1515678  A33P47VEH0YULL  B00005QG2N   \n",
       "1515690  ADG33WELAQRZJ   B00005QG2N   \n",
       "1515691  A3YXITJWFW4BW   B00005QG2N   \n",
       "1515700  A214NHULS3H7OX  B00005QG2N   \n",
       "1515701  A2IZOU2G1QX0JD  B00005QG2N   \n",
       "1515707  ARUHLQVP23GQ1   B00005QIVC   \n",
       "1515710  ARYXA0US2VB5Q   B00005QIVE   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  reviewText  \\\n",
       "1515619  This video actually focuses mostly on one of the characters that Emmanuelle (Krista Allen) is trying to teach about sex & love. It's still pretty entertaining but if you are mostly interested in Kirsta Allen then you should know that she's not really in much of this episode.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "1515627  This episode pretty much has Hafron and Emmanuelle teleporting to different parts of the world and &quot;doing it&quot;. There is the continuing plot from an earlier episode of some group on Earth trying to track them down. That's the main reason for Emmanuelle and Hafron to jump to different parts of the world. Otherwise, this episode is mostly sex scenes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "1515636  This is an intimate concert of Robert Mirabal.  Although I thought that it was, as I said, masterful, the sound, at times sounded a little muffled.The storytelling of the songs gave an insight of native culture and of Mirabal's own family stories and history.The Dance and Ee You Oo are my picks for the best songs, but they are all a joy to watch.  The Rare Tribal Mob and the Mirabal Singers/Dancers are great and provide a mesmerising stage performance.Very enjoyable                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "1515678  A well put together DVD for the \"Stinkiest band\" in the world.  Easy to navigate, and with some pretty interesting side notes...a great collectors item for any fan of Cradle of Filth or Swedish metal.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "1515690  Great show from the inventors of the extreme gothic metal genre !! 75'of pure mayhem with good visuals and a great sound, although the vocals are a little high in the mix for my taste and the guitar on the right channel is 10 times louder than the one on the left so I ended up switching it to mono to even things up. It's hard to give this more stars esp. when you've watched the PTSFirepower DVD first like I did, that one is so much better. The 5 bonus videoclips totally rule and round up the package making the purchase a worthy one.Now the mockumentary may not be suited to everyone's tastes, personally it bored me and know of a lot of people who feels it ruins the band's image. Well don't watch it then.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "1515691  I rented this dvd, i didnt buy it.  I also rented PanDamonAeon and Mannequin.  This dvd has excellent very long concert footage.  Its practically the same songs as on Livebait for the Dead.  And then we have the rest of the dvd.  Im sorry, i love CoF but that \"schockumentary\" thing was just sad.  I could barely stand to watch it.  It's stupid and boring, maybe if theyd shown more of the band in a more coherent manner...and it just goes on and on forever.  This is a 2 hour plus dvd adn half of it is crap.  The \"blair twit Project\" is slightly amusing, but mostly boring.  The music videos are good though.  I would still recommend buying this dvd just for the music videos and the concert footage.  PanDamonAeon gave me more joy though.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "1515700  Excellent Sound, Excellent Picture, Tons and Tons of songs. Love the Scorched Earth Erotic video. The only reason I didn't give this 5 stars is because I don't think they let their female singer sing enough, it adds a nice goth touch. I also did not give it five stars because it would have been nice to have the \"her ghost in the fog\" video on here, and finally I thin Dani could have done better on some parts of the songs. For example: On \"her ghost in the fog\" (live version), he chooses to growl in that typical death metal voice, instead of talking in the deep gothic voice like he does on the album. By using this growly voice, I think he is cheese'n off some of the songs best parts. Anyway, these are minor gripes and this is a must have DVD, so definitely BUY IT!...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "1515701  As most of my friends hate, I'm massively into Cradle of Filth.  In the style of Napoleon Dynamite describing a Liger, I would say \"...they're pretty much my favorite band.\"  Their music is among the worst in the industry, they suck in a way few bands can meaningfully aspire to.  I've seen them live, bought pretty much every t-shirt from every album and tour, and have almost every album that they've released over the last \"decade and a bit\".  I love it, I love it all.  I recently purchased Heavy, Left-Handed, & Candid.  If you don't like Cradle of Filth, then you'll most certainly hate this DVD.  It's absolutely fantastic, full of everything I was hoping for.There is a bunch of documentary type stuff, eleven live performances, a couple of music videos, and a few other \"shorts\" put together by the band.  Another one of the bonus items they threw on the disc is a short movie called \"The Blair Twit Project\".  This is basically footage of the band walking through woodland, filmed entirely using the night-vision mode on a video camera as they follow the trail of \"a pissed northern bastard.\"  Being a \"northerner\" myself, it felt very much like a standard night walking home from the pub on a Friday night being idiots.  It's funny to watch *once*.The DVD is actually really well put together and there is a ton of material included, which is a refreshing change when compared to some other music DVDs I've bought that barely even take the time to put a decent menu together.  As with their music, most people will hate this DVD.  I loved it, but then again, I'm weird.  The live performances are pretty awesome and it has solidified my decision to have to see this band again soon.  I don't know if they come to the US very often if at all, but some guy in Hot Topic assured me that he'd seen them here a few years ago.  I'll have to keep my eyes on the tour dates.  Then the hardest job will be convincing anyone to actually go with me.  My wife say's she's up for it, but I think that will have to be seen to be believed.   \n",
       "1515707  This category 3 classic mixes genres like only Hong Kong use to be able to do before the takeover. It over priced because of it's rarity so unless you collect rare Category 3 films you might want to get something else. As Category 3 films go this is entertaining but not as over the top as Dr. lamb, Red to Kill or the Untold Story.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
       "1515710  I remember this when i had it on video cd,it was a 2 cd set.I got because i wanted to know more about the asian culture.The story was strange,somewhat brutal & interesting.There is nudity for the guys here.I learned much from the movies.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "\n",
       "         overall  \n",
       "1515619  4.0      \n",
       "1515627  4.0      \n",
       "1515636  4.0      \n",
       "1515678  4.0      \n",
       "1515690  4.0      \n",
       "1515691  4.0      \n",
       "1515700  4.0      \n",
       "1515701  4.0      \n",
       "1515707  4.0      \n",
       "1515710  4.0      "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_dic['train']['positive'].iloc[:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do: investigate improvements to negative sentences \n",
    "For example:  \n",
    "merge negations with next word, remove next word  \n",
    "Or lemmatize based on grammar:  https://www.machinelearningplus.com/nlp/lemmatization-examples-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    lowercase=True, \n",
    "    stop_words=None, \n",
    "    max_features=MAX_FEATURES,\n",
    "    norm='l2',            # normalize each review\n",
    "    use_idf=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dummy_fun(doc):\n",
    "#     return doc\n",
    "\n",
    "# tfidf = TfidfVectorizer(\n",
    "#     analyzer='word',       # Feed a list of words to TF-IDF\n",
    "#     tokenizer=dummy_fun,\n",
    "#     preprocessor=dummy_fun,\n",
    "#     token_pattern=None,\n",
    "#     lowercase=False, \n",
    "#     stop_words=None, \n",
    "#     max_features=MAX_FEATURES,\n",
    "#     norm='l2',            # normalize each review\n",
    "#     use_idf=True)        # Keep high weight for most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words = pd.concat([movie_reviews['train']['positive']['reviewText'],\n",
    "                     movie_reviews['train']['negative']['reviewText']])\n",
    "y_train = np.concatenate([np.ones((movie_reviews['train']['positive'].shape[0],)), \n",
    "                          np.zeros((movie_reviews['train']['negative'].shape[0],))])\n",
    "test_words = pd.concat([movie_reviews['test']['positive']['reviewText'],\n",
    "                     movie_reviews['test']['negative']['reviewText']])\n",
    "y_test = np.concatenate([np.ones((movie_reviews['test']['positive'].shape[0],)), \n",
    "                          np.zeros((movie_reviews['test']['negative'].shape[0],))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_words = pd.concat([movie_reviews['train']['positive']['words'],\n",
    "#                      movie_reviews['train']['negative']['words']])\n",
    "# y_train = np.concatenate([np.ones((movie_reviews['train']['positive'].shape[0],)), \n",
    "#                           np.zeros((movie_reviews['train']['negative'].shape[0],))])\n",
    "# test_words = pd.concat([movie_reviews['test']['positive']['words'],\n",
    "#                      movie_reviews['test']['negative']['words']])\n",
    "# y_test = np.concatenate([np.ones((movie_reviews['test']['positive'].shape[0],)), \n",
    "#                           np.zeros((movie_reviews['test']['negative'].shape[0],))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPARSE = True\n",
    "\n",
    "if SPARSE:\n",
    "    # Optimization: add the review length while keeping sparse matrix\n",
    "    tf_train = tfidf.fit_transform(train_words)\n",
    "    tf_test = tfidf.transform(test_words)\n",
    "else:\n",
    "    tf_train = tfidf.fit_transform(train_words).todense()\n",
    "    tf_test = tfidf.transform(test_words).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(tfidf.vocabulary_))\n",
    "# tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add review length to modeling input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADD_LENGTH = False\n",
    "\n",
    "if ADD_LENGTH:\n",
    "    if SPARSE:\n",
    "        # Hack: pick an existing word to store the count\n",
    "        len_idx = 0\n",
    "        test_lengths = [len(words) for words in test_words]\n",
    "\n",
    "        for idx,words in enumerate(train_words):\n",
    "            tf_train[idx][len_idx] = len(words)\n",
    "        for idx,words in enumerate(test_words):\n",
    "            tf_test[idx][len_idx] = len(words)\n",
    "        X_train = tf_train\n",
    "        X_test = tf_test\n",
    "    else:\n",
    "        train_lengths = np.array([len(words) for words in train_words]).reshape(-1,1)\n",
    "        test_lengths = np.array([len(words) for words in test_words]).reshape(-1,1)\n",
    "        X_train = np.concatenate([tf_train, train_lengths],axis=1)\n",
    "        X_test = np.concatenate([tf_test, test_lengths],axis=1)\n",
    "else:\n",
    "    X_train = tf_train\n",
    "    X_test = tf_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if X_train.shape[0] != y_train.shape[0] or X_test.shape[0] != y_test.shape[0]:\n",
    "    print('@@@ Problem! @@@')\n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "    print(X_test.shape)\n",
    "    print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    pickle_out = open(data_path \n",
    "                      + 'tfidf_' \n",
    "                      + str(X_train.shape[0]) + 'Pos_Neg_Samples_'\n",
    "                      + str(X_train.shape[1]) + 'Feats.pkl'\n",
    "                      ,\"wb\")\n",
    "    pickle.dump(tfidf, pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Classifier for Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Classifier parameters\n",
    "# N_TREES = math.floor(np.sqrt(NB_SAMPLES) * 1.2)\n",
    "N_TREES = 500\n",
    "LEARN_RATE = 0.2\n",
    "MAX_DEPTH = 8\n",
    "MIN_IN_LEAF = 5 #7\n",
    "MAX_FEATURES = 'sqrt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier(learning_rate=LEARN_RATE, \n",
    "                                n_estimators=N_TREES, \n",
    "                                min_samples_leaf=MIN_IN_LEAF,\n",
    "                                max_depth=MAX_DEPTH,\n",
    "                                max_features=MAX_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    pickle.dump(gbc, open(data_path + 'GBC_'\n",
    "                       + str(NB_SAMPLES) + '_samples_'\n",
    "                       + str(N_TREES) + '_trees_' \n",
    "                       + str(LEARN_RATE) + '_lr_' \n",
    "                       + str(MAX_DEPTH) + '_maxdpth_'\n",
    "                       + str(MIN_IN_LEAF) + '_minleaf_'\n",
    "                       + str(MAX_FEATURES) + '_feats_'\n",
    "                       + '.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "print(MAX_FEATURES, ' features', N_TREES,'trees; ',\n",
    "      LEARN_RATE,'learn_rate; ', MAX_DEPTH, 'max_dpth; ',\n",
    "      MIN_IN_LEAF, 'min_in_leaf')\n",
    "classifier_report(gbc, X_train, y_train,\n",
    "                  'Gradient Boosting Classifier on training set')\n",
    "classifier_report(gbc, X_test, y_test, \n",
    "                  'Gradient Boosting Classifier on test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SAMPLE_FRACTION:', SAMPLE_FRACTION,'ADD_LENGTH:',ADD_LENGTH,' SPARSE:',SPARSE,' MAX_FEATURES:',MAX_FEATURES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model tuning\n",
    "Find the maximum size the machine will take (samples * features)\n",
    "Accuracy increases by 2% when increasing dataset from 36K to 72K samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate what went wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = gbc.predict(X_test)\n",
    "true_0_pred_1 = (test_pred == 1) & (y_test == 0)\n",
    "print(np.unique(true_0_pred_1, return_counts=True))\n",
    "print(len(true_0_pred_1))\n",
    "print(len(X_test))\n",
    "X_test[true_0_pred_1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reviews = pd.concat([movie_reviews['test']['positive'],\n",
    "                     movie_reviews['test']['negative']])\n",
    "test_reviews[true_0_pred_1]['reviewText'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    gb_pipe = Pipeline([('vect', tfidf), ('gb', gbc)])\n",
    "    gb_pipe.fit(X_train, y_train)\n",
    "    pickle.dump(gb_pipe, open('pickles/GBCpipe_balanced_comments_'\n",
    "                           + str(N_TREES) + '_trees_' \n",
    "                           + str(LEARN_RATE) + '_lr_' \n",
    "                           + str(MAX_DEPTH) + '_maxdpth_'\n",
    "                           + str(MIN_IN_LEAF) + '_minleaf_'\n",
    "                           + str(MAX_FEATURES) + '_feats_'\n",
    "                           + '.pkl', 'wb'))\n",
    "else:\n",
    "#     pickle_in = open(\"pickles/GBC_balanced_comments_300_trees_0.1_lr_15_maxdpth_2_minleaf_20000_feats_.pkl\",\n",
    "#                      \"rb\")\n",
    "#     gb_pipe = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    grid = {\n",
    "        'learning_rate': [.1,0.2,0.3],\n",
    "        'max_depth': [8],\n",
    "        'min_samples_leaf': [5],\n",
    "        'max_features': ['sqrt'],\n",
    "        'n_estimators': [300],\n",
    "        'random_state': [0]\n",
    "    }\n",
    "else:  # TEST\n",
    "    grid = {\n",
    "    'learning_rate': [1],\n",
    "    'max_depth': [2], \n",
    "    'min_samples_leaf': [2],\n",
    "#     'max_features': ['sqrt', None],\n",
    "    'n_estimators': [2],\n",
    "    'random_state': [0]\n",
    "}\n",
    "    \n",
    "# confusion_score = make_scorer(confusion_rmse, greater_is_better=False)\n",
    "\n",
    "gbc_grid_cv = GridSearchCV(\n",
    "    GradientBoostingClassifier(), \n",
    "    grid,\n",
    "    cv=4,  # number of folds\n",
    "    return_train_score=True,\n",
    "    verbose=1, \n",
    "    n_jobs=-1)\n",
    "gbc_grid_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gbc_grid_cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SAMPLE_FRACTION:', SAMPLE_FRACTION,'ADD_LENGTH:',ADD_LENGTH,' SPARSE:',SPARSE,' MAX_FEATURES:',MAX_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gbc_grid_cv.best_params_)\n",
    "print(gbc_grid_cv.best_score_)\n",
    "res_df = pd.DataFrame(gbc_grid_cv.cv_results_)\n",
    "res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case B: with objective sentences removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split comments into separate sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "small['sentence'] = small['reviewText'].map(sent_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small.drop(['reviewerName', 'helpful', 'reviewText', 'summary', \n",
    "            'unixReviewTime', 'reviewTime'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = small['sentence'] \\\n",
    ".apply(pd.Series) \\\n",
    ".merge(small, left_index = True, right_index = True) \\\n",
    ".drop(['sentence'], axis = 1) \\\n",
    ".melt(id_vars = ['reviewerID', 'asin','overall'], value_name = 'sentence') \\\n",
    ".drop(['variable'], axis = 1) \\\n",
    ".dropna()\n",
    "\n",
    "print(sentences.shape)\n",
    "sentences.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence-level prep & cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "from utils import split_n_lower, not_about_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into words and lower the case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences['words'] = sentences['sentence'].apply(lambda s: split_n_lower(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences.shape)\n",
    "sentences.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep support-related sentences as they probably have impact on rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on_movies_filter = [not_about_support(word) for word in sentences['words']]\n",
    "sentences_on_movie = sentences #[on_movies_filter]\n",
    "\n",
    "print('Removing {} records'.format(sentences.shape[0]- sentences_on_movie.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_on_movie.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base case: A reviews with objective and subjective sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel dies here at 50K samples\n",
    "all_reviews_groups = sentences_on_movie.groupby(['reviewerID','asin'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews_stars = all_reviews_groups['overall'].mean()\n",
    "all_reviews_stars[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews_comments = all_reviews_groups['words'].sum()\n",
    "print(sentences_on_movie.iloc[0, 4])\n",
    "print(all_reviews_comments.shape)\n",
    "print(all_reviews_comments[0])\n",
    "len(all_reviews_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove objective sentences for case B using obj-subj model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_colwidth', -1)\n",
    "# sentences_on_movie['sentence']\n",
    "sentences_on_movie.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_on_movie.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize along the word space of the obj-subj training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = pickle.load(open('pickles/Obj-Subj_tfidf.pkl', 'rb'))\n",
    "len(tfidf.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_tfidf = tfidf.transform(sentences_on_movie['sentence']).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply the obj-subj model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TREES = 100\n",
    "LEARN_RATE = 0.1\n",
    "MIN_IN_LEAF = 10\n",
    "pickle_in = open('pickles/GBC_'+ str(N_TREES) +'_' + str(LEARN_RATE) \n",
    "                        +'_' + str(MIN_IN_LEAF) + '_20min.pkl', 'rb')\n",
    "gb_model = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = gb_model.predict(sentences_tfidf)\n",
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjective_sentences = sentences_on_movie[y_test == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown('### Removing {} objective sentences'\n",
    "                 .format(len(y_test) - len(subjective_sentences))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjective_sentences.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge the sentences back into paragraph reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_groups = subjective_sentences.groupby(['reviewerID','asin'])\n",
    "subj_reviews_stars = subj_groups['overall'].mean()\n",
    "# subjective_reviewssubjective_reviews['sentence'].apply(lambda x: x.sum())\n",
    "# subjective_reviews_reviews = \n",
    "subj_reviews_stars[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_review_comments = subj_groups['words'].sum()\n",
    "print(subj_review_comments.shape)\n",
    "print(subj_review_comments[0])\n",
    "subj_review_comments[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that stars still correspond to the right movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 6000\n",
    "end = 6010\n",
    "all_reviews_comments.loc[('A33Z7JTV7SSW9Y', '0718000315')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_reviews_stars.loc[('A33Z7JTV7SSW9Y', '0718000315')])\n",
    "print(sentences_on_movie.loc[sentences_on_movie['reviewerID']=='A33Z7JTV7SSW9Y']) \n",
    "# and sentences_on_movie['asin']=='0718000315'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = -1\n",
    "print(small.loc[small['reviewerID']=='A33Z7JTV7SSW9Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_on_movie[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create emotion vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of reviews:', all_reviews_comments.shape[0])\n",
    "print('Total number of subjective reviews:', subj_review_comments.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "from emotions_seven import Emotions7\n",
    "emote = Emotions7()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews_emotions = emote.vectorize(all_reviews_comments)\n",
    "print(all_reviews_emotions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emote.emotions_in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews_emotions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_revs_with_emotions = all_reviews_emotions[emote.emotions_in_text == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(all_revs_with_emotions.shape)\n",
    "# all_revs_stars = all_reviews_stars[emote.emotions_in_text]\n",
    "all_reviews_emotions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_reviews_emotions = emote.vectorize(subj_review_comments)\n",
    "print(subj_reviews_emotions.shape)\n",
    "subj_reviews_emotions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a model on base case (all comments) for star rating prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_subj_train, X_subj_cv, y_subj_train, y_subj_cv = train_test_split(\n",
    "    subj_reviews_emotions, subj_reviews_stars, test_size=0.2, random_state=0)\n",
    "X_subj_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_subj = GradientBoostingClassifier(learning_rate=LEARN_RATE, \n",
    "                                n_estimators=N_TREES, \n",
    "                                min_samples_leaf=MIN_IN_LEAF,\n",
    "                                random_state=0)\n",
    "gbc_subj.fit(X_subj_train, y_subj_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Gradient Boosting Classifier')\n",
    "print('Training score using all comments: {0:.2f}'\n",
    "      .format(gbc_all.score(X_train, y_train)))\n",
    "print('CV score using all comments: {0:.2f}'\n",
    "      .format(gbc_all.score(X_cv, y_cv)))\n",
    "print('')\n",
    "\n",
    "# print('Training score using subjective comments only: {0:.2f}'\n",
    "#       .format(gbc_subj.score(X_subj_train, y_subj_train)))\n",
    "# print('CV score using subjective comments only: {0:.2f}'\n",
    "#       .format(gbc_subj.score(X_subj_cv, y_subj_cv)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# lr = LogisticRegression(random_state=0, solver='lbfgs',\n",
    "#                        multi_class='multinomial',max_iter=1000)\n",
    "# lr.fit(X_subj_train, y_subj_train)\n",
    "# print(lr.score(X_subj_train, y_subj_train))\n",
    "# print(lr.score(X_subj_cv, y_subj_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_all = sm.OLS(y_train, X_train)\n",
    "results_all = ols_all.fit()\n",
    "results_all.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ols_subj = sm.OLS(y_subj_train, X_subj_train)\n",
    "results_subj = ols_subj.fit()\n",
    "results_subj.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "all_reviews_emotions, all_reviews_stars\n",
    "\n",
    "sns.heatmap(raw_df.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
